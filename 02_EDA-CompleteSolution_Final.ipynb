{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "640bd3f7",
   "metadata": {},
   "source": [
    "## Initial Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "04ed2518",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2489fc76acd4a148446d2e86969b58a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>0</td><td>application_1693814926276_0001</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-10-0-5-13.ec2.internal:20888/proxy/application_1693814926276_0001/\" class=\"emr-proxy-link\" emr-resource=\"j-RPUVU1R2JDGI\n",
       "\" application-id=\"application_1693814926276_0001\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-10-0-5-13.ec2.internal:8042/node/containerlogs/container_1693814926276_0001_01_000001/livy\" >Link</a></td><td>None</td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "494d9d27",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74279f5f1b4a4e0fbfb7208f9ad9aac5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"Basics\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a3d6550",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f4141b56dd84813b9ee961880e2562f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.sql.session.SparkSession object at 0x7f13c89ec550>"
     ]
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fc5e0bec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fac1c802f76e4175b60a53872cf8c4e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting plotly==5.5.0\n",
      "  Downloading plotly-5.5.0-py2.py3-none-any.whl (26.5 MB)\n",
      "Collecting tenacity>=6.2.0\n",
      "  Downloading tenacity-8.2.3-py3-none-any.whl (24 kB)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.7/site-packages (from plotly==5.5.0) (1.13.0)\n",
      "Installing collected packages: tenacity, plotly\n",
      "Successfully installed plotly-5.5.0 tenacity-8.2.3\n",
      "\n",
      "Collecting pandas==0.25.1\n",
      "  Downloading pandas-0.25.1-cp37-cp37m-manylinux1_x86_64.whl (10.4 MB)\n",
      "Collecting python-dateutil>=2.6.1\n",
      "  Downloading python_dateutil-2.8.2-py2.py3-none-any.whl (247 kB)\n",
      "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/site-packages (from pandas==0.25.1) (2022.7)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib64/python3.7/site-packages (from pandas==0.25.1) (1.20.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/site-packages (from python-dateutil>=2.6.1->pandas==0.25.1) (1.13.0)\n",
      "Installing collected packages: python-dateutil, pandas\n",
      "Successfully installed pandas-0.25.1 python-dateutil-2.8.2\n",
      "\n",
      "Collecting numpy==1.14.5\n",
      "  Downloading numpy-1.14.5-cp37-cp37m-manylinux1_x86_64.whl (12.2 MB)\n",
      "Installing collected packages: numpy\n",
      "Successfully installed numpy-1.14.5\n",
      "\n",
      "Collecting matplotlib==3.1.1\n",
      "  Downloading matplotlib-3.1.1-cp37-cp37m-manylinux1_x86_64.whl (13.1 MB)\n",
      "Collecting cycler>=0.10\n",
      "  Downloading cycler-0.11.0-py3-none-any.whl (6.4 kB)\n",
      "Collecting pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1\n",
      "  Downloading pyparsing-3.1.1-py3-none-any.whl (103 kB)\n",
      "Requirement already satisfied: numpy>=1.11 in ./tmp/1693815687611-0/lib/python3.7/site-packages (from matplotlib==3.1.1) (1.14.5)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in ./tmp/1693815687611-0/lib/python3.7/site-packages (from matplotlib==3.1.1) (2.8.2)\n",
      "Collecting kiwisolver>=1.0.1\n",
      "  Downloading kiwisolver-1.4.5-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.1 MB)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/site-packages (from python-dateutil>=2.1->matplotlib==3.1.1) (1.13.0)\n",
      "Collecting typing-extensions; python_version < \"3.8\"\n",
      "  Downloading typing_extensions-4.7.1-py3-none-any.whl (33 kB)\n",
      "Installing collected packages: cycler, pyparsing, typing-extensions, kiwisolver, matplotlib\n",
      "Successfully installed cycler-0.11.0 kiwisolver-1.4.5 matplotlib-3.1.1 pyparsing-3.1.1 typing-extensions-4.7.1\n",
      "\n",
      "WARNING: The directory '/home/.cache/pip' or its parent directory is not owned or is not writable by the current user. The cache has been disabled. Check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.\n",
      "\n",
      "WARNING: The directory '/home/.cache/pip' or its parent directory is not owned or is not writable by the current user. The cache has been disabled. Check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.\n",
      "\n",
      "WARNING: The directory '/home/.cache/pip' or its parent directory is not owned or is not writable by the current user. The cache has been disabled. Check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.\n",
      "ERROR: After October 2020 you may experience errors when installing or updating packages. This is because pip will change the way that it resolves dependency conflicts.\n",
      "\n",
      "We recommend you use --use-feature=2020-resolver to test your packages with the new resolver before it becomes the default.\n",
      "\n",
      "python37-sagemaker-pyspark 1.4.2 requires pyspark==2.4.0, which is not installed.\n",
      "\n",
      "WARNING: The directory '/home/.cache/pip' or its parent directory is not owned or is not writable by the current user. The cache has been disabled. Check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag."
     ]
    }
   ],
   "source": [
    "# Run this everytime you create a new spark instance. \n",
    "\n",
    "spark.sparkContext.install_pypi_package(\"plotly==5.5.0\")\n",
    "spark.sparkContext.install_pypi_package(\"pandas==1.0.5\")\n",
    "spark.sparkContext.install_pypi_package(\"numpy==1.14.5\")\n",
    "spark.sparkContext.install_pypi_package(\"matplotlib==3.1.1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d956d7bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "354c0308a8d1452993aecdc02378b19f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.ml.feature import Bucketizer\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Import for typecasting columns\n",
    "from pyspark.sql.types import IntegerType,BooleanType,DateType,FloatType,StringType\n",
    "from pyspark.sql.types import ArrayType"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c41efa",
   "metadata": {},
   "source": [
    "## Defining Custom Functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "922a9b82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d04a1fdbabd64d6bb094a6ec30e57ea8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def get_quantiles(df, col_name, quantiles_list = [0.01, 0.25, 0.5, 0.75, 0.99]):\n",
    "    \"\"\"\n",
    "    Takes a numerical column and returns column values at requested quantiles\n",
    "\n",
    "    Inputs \n",
    "    Argument 1: Dataframe\n",
    "    Argument 2: Name of the column\n",
    "    Argument 3: A list of quantiles you want to find. Default value [0.01, 0.25, 0.5, 0.75, 0.99]\n",
    "\n",
    "    Output \n",
    "    Returns a dictionary with quantiles as keys and column quantile values as values \n",
    "    \"\"\"\n",
    "    # Get min, max and quantile values for given column\n",
    "    min_val = df.agg(F.min(col_name)).first()[0]\n",
    "    max_val = df.agg(F.max(col_name)).first()[0]\n",
    "    quantiles_vals = df.approxQuantile(col_name,\n",
    "                                       quantiles_list,\n",
    "                                       0)\n",
    "  \n",
    "    # Store min, quantiles and max in output dict, sequentially\n",
    "    quantiles_dict = {0.0:min_val}\n",
    "    quantiles_dict.update(dict(zip(quantiles_list, quantiles_vals)))\n",
    "    quantiles_dict.update({1.0:max_val})\n",
    "    return(quantiles_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "da0aa680",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f7739922a01440d8cdfe012075e8777",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "def plot_bucketwise_statistics (summary, bucketizer):\n",
    "    \"\"\"\n",
    "    Takes in a dataframe and a bucketizer object and plots the summary statistics for each bucket in the dataframe. \n",
    "  \n",
    "    Inputs\n",
    "    Argument 1: Pandas dataframe obtained from bucket_col_print_summary function \n",
    "    Argument 2: Bucketizer object obtained from bucket_col_print_summary function\n",
    "  \n",
    "    Output\n",
    "    Displays a plot of bucketwise average ratings nunber of ratings of a parameter.   \n",
    "    \"\"\"\n",
    "    # Creating bucket labels from splits\n",
    "    classlist = bucketizer.getSplits()\n",
    "    number_of_classes = len(classlist) - 1\n",
    "\n",
    "    class_labels = []\n",
    "    hover_labels = []\n",
    "    for i in range (number_of_classes):\n",
    "        hover_labels.append(str(classlist[i])+\"-\"+str(classlist[i+1]) +\" (Bucket name: \"+ str(int(i)) +\")\"  )\n",
    "        class_labels.append(str(classlist[i])+\"-\"+str(classlist[i+1]) )\n",
    "  \n",
    "    summary[\"Scaled_number\"] = (summary[\"n_ratings\"]-summary[\"n_ratings\"].min())/(summary[\"n_ratings\"].max()-summary[\"n_ratings\"].min()) + 1.5\n",
    "    summary['Bucket_Names'] = class_labels\n",
    "  \n",
    "    # making plot\n",
    "    x = summary[\"Bucket_Names\"]\n",
    "    y1 = summary[\"avg_rating\"]\n",
    "    y2 = summary[\"n_ratings\"]\n",
    "    err = summary[\"stddev_rating\"]  \n",
    "\n",
    "    # Plot scatter here\n",
    "    plt.rcParams[\"figure.figsize\"] = [summary.shape[0]+2, 6.0]\n",
    "    plt.rcParams[\"figure.autolayout\"] = True\n",
    "    fig, ax1 = plt.subplots()\n",
    "\n",
    "    bar = ax1.bar(x, y1, color = \"#262261\")\n",
    "    ax1.errorbar(x, y1, yerr=err, fmt=\"o\", color=\"#EE4036\")\n",
    "    ax1.set(ylim=(0, 7))\n",
    "  \n",
    "    #ax1.bar_label(bar , fmt='%.2f', label_type='edge')  \n",
    "    def barlabel(x_list,y_list):\n",
    "        for i in range(len(x_list)):\n",
    "            ax1.text(i,y_list[i] + 0.2,y_list[i], ha = 'center',\n",
    "  \t\t\t         fontdict=dict(size=10),\n",
    "  \t\t\t         bbox=dict(facecolor='#262261', alpha=0.2)         \n",
    "  \t\t\t        )\n",
    "    barlabel(summary[\"Bucket_Names\"].tolist() ,summary[\"avg_rating\"].round(2).tolist())\n",
    "  \n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.scatter(x, y2, s=summary[\"Scaled_number\"]*500, c = '#FAAF40')  \n",
    "    ax2.set(ylim=(0, summary[\"n_ratings\"].max()*1.15))\n",
    "    def scatterlabel(x_list,y_list):\n",
    "  \t    for i in range(len(x_list)):\n",
    "  \t\t    ax2.text(i,y_list[i] + 15000,y_list[i], ha = 'center',\n",
    "  \t\t\t\t\t fontdict=dict(size=10),\n",
    "                     bbox=dict(facecolor='#FAAF40', alpha=0.5)\n",
    "  \t\t\t\t\t)\n",
    "    scatterlabel(summary[\"Bucket_Names\"].tolist() ,summary[\"n_ratings\"].tolist())\n",
    "  \n",
    "    # giving labels to the axises\n",
    "    ax1.set_xlabel(bucketizer.getOutputCol(), fontdict=dict(size=14)) \n",
    "    ax1.set_ylabel(\"Average Ratings\",fontdict=dict(size=14))\n",
    "  \n",
    "    # secondary y-axis label\n",
    "    ax2.set_ylabel('Number of Ratings',fontdict=dict(size=14))\n",
    "  \n",
    "    #plot Title\n",
    "    plt.title('Bucketwise average ratings and number of ratings for \\n'+bucketizer.getInputCol(), \n",
    "              fontdict=dict(size=14))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "32067450",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "002c9df83cf44da887d171154145a0cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def bucket_col_print_summary(df, splits, inputCol, outputCol):\n",
    "    \"\"\"\n",
    "    Given a numerical column in a data frame, adds a bucketized version of the column to the data frame, according to splits provided.\n",
    "    Also prints a summary of ratings seen in each bucket made.\n",
    "\n",
    "    Inputs \n",
    "    Argument 1: Data Frame \n",
    "    Argument 2: Values at which the column will be split\n",
    "    Argument 3: Name of the input column (numerical column)\n",
    "    Argument 4: Name of the output column (bucketized numerical column)\n",
    "\n",
    "    Output: \n",
    "    1) New dataframe with the output column added\n",
    "    2) Bucketizer object trained from the input column \n",
    "    3) Pandas dataframe with summary statistics for ratings seen in buckets of the output column\n",
    "    Also plots summary statistics for ratings seen in buckets of the output column\n",
    "    \"\"\"\n",
    "\n",
    "    # Dropping bucket if it already exists\n",
    "    if outputCol in df.columns:\n",
    "        df = df.drop(outputCol)\n",
    "\n",
    "    # Training bucketizer\n",
    "    bucketizer = Bucketizer(splits = splits,\n",
    "                            inputCol  = inputCol,\n",
    "                            outputCol = outputCol)\n",
    "    \n",
    "    df = bucketizer.setHandleInvalid(\"keep\").transform(df)\n",
    "\n",
    "    # Printing meta information on buckets created\n",
    "    print(\"Added bucketized column {}\".format(outputCol))\n",
    "    print(\"\")\n",
    "    print(\"Bucketing done for split definition: {}\".format(splits))\n",
    "    print(\"\")  \n",
    "    print(\"Printing summary statistics for ratings in buckets below:\")\n",
    "\n",
    "    # Creating a summary statistics dataframe and passing it to the plotting function\n",
    "    summary =  (df\n",
    "                .groupBy(outputCol)\n",
    "                .agg(F.avg('rating').alias('avg_rating'),\n",
    "                     F.stddev('rating').alias('stddev_rating'),\n",
    "                     F.count('rating').alias('n_ratings'))\n",
    "                .sort(outputCol)\n",
    "                .toPandas())\n",
    "  \n",
    "    plot_bucketwise_statistics(summary,bucketizer)\n",
    "  \n",
    "    return df, bucketizer, summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2844741f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d015645778849dc80e6beb08ba7cb39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def get_column_distribution_summary(df, col_name):\n",
    "    \"\"\"\n",
    "    Takes a column in a data frame and prints the summary statistics (average, standard deviation, count and distinct count) for all unique values in that column.\n",
    "  \n",
    "    Inputs \n",
    "    Argument 1: Dataframe \n",
    "    Argument 2: Name of the column\n",
    "  \n",
    "    Output\n",
    "    Returns nothing \n",
    "    Prints a Dataframe with summary statistics\n",
    "    \"\"\"\n",
    "    print(df\n",
    "          .groupBy(col_name)\n",
    "          .agg(F.avg('rating').alias('avg_rating'),\n",
    "               F.stddev('rating').alias('stddev_rating'),\n",
    "               F.count('rating').alias('n_ratings'),\n",
    "               F.countDistinct('id').alias('n_recipes'))\n",
    "          .sort(F.col(col_name).asc())\n",
    "          .show(50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9e4067ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4be3e5cba274b99b108e8ca11e12dcc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def get_n_items_satisfying_condition (df, condition, aggregation_level = \"recipe\"):\n",
    "    \"\"\"\n",
    "    Given a condition, find the number of recipes / reviews that match the condition.\n",
    "    Also calculates the percentage of such recipes / reviews as a percentage of all recipes / reviews.\n",
    "  \n",
    "    Inputs \n",
    "    Argument 1: Dataframe \n",
    "    Argument 2: Logical expression describing a condition, string type. eg: \"minutes == 0\"\n",
    "    Argument 3: Aggregation level for determining \"items\", either  \"recipe\" or \"review\". Default value == \"recipe\"\n",
    "  \n",
    "    Output: Returns no object.\n",
    "    Prints the following:\n",
    "    1) Number of recipes / reviews that satisfy the condition\n",
    "    2) Total number of recipes / reviews in the dataframe\n",
    "    3) Percentage of recipes / reviews that satisfy the condition\n",
    "    \"\"\"\n",
    "    # Find out num rows satisfying the condition\n",
    "    if aggregation_level == \"recipe\": \n",
    "        number_of_rows_satisfying_condition = (df\n",
    "                                             .filter(condition)\n",
    "                                             .agg(F.countDistinct(\"id\"))).first()[0]\n",
    "      \n",
    "        n_rows_total = (df.agg(F.countDistinct(\"id\"))).first()[0]\n",
    "    if aggregation_level == \"review\":\n",
    "        number_of_rows_satisfying_condition = (df\n",
    "                                             .filter(condition)\n",
    "                                             .agg(F.countDistinct(\"id\",\"user_id\"))).first()[0]\n",
    "        n_rows_total = (df.agg(F.countDistinct(\"id\",\"user_id\"))).first()[0]\n",
    "  \n",
    "    # Find out % rows satisfying the conditon and print a properly formatted output\n",
    "    perc_rows = round(number_of_rows_satisfying_condition * 100/ n_rows_total, 2)\n",
    "    print('Condition String                   : \"{}\"'.format(condition))\n",
    "    print(\"Num {}s Satisfying Condition   : {} [{}%]\".format(aggregation_level.title(), number_of_rows_satisfying_condition, perc_rows))\n",
    "    print(\"Total Num {}s                  : {}\".format(aggregation_level.title(), n_rows_total))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eefe5257",
   "metadata": {},
   "source": [
    "## Read the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca2574e",
   "metadata": {},
   "source": [
    "- Read ```interaction_level_df_processed```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "701e66a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b8d5376b5aa427d98a5bae1512ac460",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "interaction_level_df = spark.read.parquet(\"s3://demobucketpavi/foodreceipe/interaction_level_df\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7acf36fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d452858c76554147a53f0ccb40c180b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Code check cell\n",
    "# Do not edit cells with assert commands\n",
    "# If an error is shown after running this cell, please recheck your code.  \n",
    "\n",
    "assert interaction_level_df.count() == 1132367, \"There is a mistake in reading the data.\"\n",
    "assert len(interaction_level_df.columns) == 33, \"There is a mistake in reading the data.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6ff3ae16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "012b16b65ceb4775a41970143e011107",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+------+--------------------+-----------+--------------------+-------+--------------+----------+--------------------+--------------------+-------+--------------------+--------------------+--------------------+-------------+--------+-------------+---------+----------+-----------+-----------------+-----------------+--------------------+---------------------+------------------+------------------+-------------------+-------------------------+-------------------------+------------------------------------+--------------------------------------+-------------------------------------+\n",
      "|user_id|recipe_id|rating|              review|review_date|                name|minutes|contributor_id| submitted|                tags|           nutrition|n_steps|               steps|         description|         ingredients|n_ingredients|calories|total_fat_PDV|sugar_PDV|sodium_PDV|protein_PDV|saturated_fat_PDV|carbohydrates_PDV|calories_per_100_cal|total_fat_per_100_cal| sugar_per_100_cal|sodium_per_100_cal|protein_per_100_cal|saturated_fat_per_100_cal|carbohydrates_per_100_cal|days_since_submission_on_review_date|months_since_submission_on_review_date|years_since_submission_on_review_date|\n",
      "+-------+---------+------+--------------------+-----------+--------------------+-------+--------------+----------+--------------------+--------------------+-------+--------------------+--------------------+--------------------+-------------+--------+-------------+---------+----------+-----------+-----------------+-----------------+--------------------+---------------------+------------------+------------------+-------------------+-------------------------+-------------------------+------------------------------------+--------------------------------------+-------------------------------------+\n",
      "| 557764|       53|     3|First Jimmy let m...| 2009-07-09|jimmy g s carrot ...|    110|          1772|1999-09-08|[weeknight,  time...|372.9, 16.0, 163....|     15|['grease and ligh...|                    |['all-purpose flo...|           13|   372.9|         16.0|    163.0|       7.0|       13.0|             16.0|             22.0|  100.00000163677011|    4.290694626410088| 43.71145150655277|1.8771788990544134| 3.4861893839581963|        4.290694626410088|        5.899705111313871|                                3592|                          118.03225806|                          9.836021505|\n",
      "|  27678|       85|     5|A perfect cake. I...| 2005-03-25| butter madeira cake|    175|          1543|1999-09-12|[weeknight,  time...|373.8, 30.0, 88.0...|     15|['grease lined ti...|this is a good st...|['flour', 'baking...|            7|   373.8|         30.0|     88.0|      10.0|       11.0|             59.0|             14.0|  100.00000326565855|     8.02568244507693| 23.54200183889233|  2.67522748169231|  2.942750229861541|       15.783842141984628|        3.745318474369234|                                2021|                           66.41935484|                    5.534946236666666|\n",
      "| 273254|       85|     5|This recipe is so...| 2009-02-09| butter madeira cake|    175|          1543|1999-09-12|[weeknight,  time...|373.8, 30.0, 88.0...|     15|['grease lined ti...|this is a good st...|['flour', 'baking...|            7|   373.8|         30.0|     88.0|      10.0|       11.0|             59.0|             14.0|  100.00000326565855|     8.02568244507693| 23.54200183889233|  2.67522748169231|  2.942750229861541|       15.783842141984628|        3.745318474369234|                                3438|                          112.90322581|                    9.408602150833334|\n",
      "|  92666|       91|     4|I'm not a big fan...| 2004-01-30|brown rice and ve...|    150|          1576|1999-09-06|[weeknight,  time...|412.6, 37.0, 10.0...|      9|['preheat oven to...|this is good with...|['brown rice', 'c...|           13|   412.6|         37.0|     10.0|      17.0|       16.0|             81.0|             14.0|   99.99999852071848|     8.96752289206637|2.4236548356936134| 4.120213220679142| 3.8778477371097813|        19.63160416911827|       3.3931167699710585|                                1607|                           52.77419355|                         4.3978494625|\n",
      "| 189393|       91|     5|Wonderful way to ...| 2005-05-17|brown rice and ve...|    150|          1576|1999-09-06|[weeknight,  time...|412.6, 37.0, 10.0...|      9|['preheat oven to...|this is good with...|['brown rice', 'c...|           13|   412.6|         37.0|     10.0|      17.0|       16.0|             81.0|             14.0|   99.99999852071848|     8.96752289206637|2.4236548356936134| 4.120213220679142| 3.8778477371097813|        19.63160416911827|       3.3931167699710585|                                2080|                           68.35483871|                    5.696236559166667|\n",
      "+-------+---------+------+--------------------+-----------+--------------------+-------+--------------+----------+--------------------+--------------------+-------+--------------------+--------------------+--------------------+-------------+--------+-------------+---------+----------+-----------+-----------------+-----------------+--------------------+---------------------+------------------+------------------+-------------------+-------------------------+-------------------------+------------------------------------+--------------------------------------+-------------------------------------+\n",
      "only showing top 5 rows"
     ]
    }
   ],
   "source": [
    "interaction_level_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb8336fb",
   "metadata": {},
   "source": [
    "## Bucketing and Cleaning Numerical Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a259fc10",
   "metadata": {},
   "source": [
    "#### **1. `years_since_submission_on_review_date`** \n",
    "[Review Time Since Submission]\n",
    "- Recipes more than 6 years old are rated low"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9739318a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf3da3ef04034d44b3e50f36e1720970",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0.0: -4.833333333333333, 0.01: 0.005376344166666667, 0.25: 0.8602150541666668, 0.5: 2.9489247308333333, 0.75: 5.844086021666667, 0.99: 14.626344085833333, 1.0: 19.284946236666666}"
     ]
    }
   ],
   "source": [
    "get_quantiles(df = interaction_level_df,\n",
    "              col_name = \"years_since_submission_on_review_date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1c5d0a1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e35ea3687a84bc5a427ea829de6c204",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Condition String                   : \"years_since_submission_on_review_date < 0\"\n",
      "Num Reviews Satisfying Condition   : 1246 [0.11%]\n",
      "Total Num Reviews                  : 1132367"
     ]
    }
   ],
   "source": [
    "get_n_items_satisfying_condition(df = interaction_level_df,\n",
    "                                 condition= 'years_since_submission_on_review_date < 0',\n",
    "                                 aggregation_level= \"review\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8015a401",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a078bfb7119e4eb9acd005e71a0f999c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Only keep interactions with review dates >= recipe submission date\n",
    "\n",
    "interaction_level_df = (interaction_level_df\n",
    "                        .filter('years_since_submission_on_review_date >= 0'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7cd92e5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9df47302f584bdfb5312a1a72896ba9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "Pandas >= 1.0.5 must be installed; however, your version was 0.25.1.\n",
      "Traceback (most recent call last):\n",
      "  File \"<stdin>\", line 43, in bucket_col_print_summary\n",
      "  File \"/mnt/yarn/usercache/livy/appcache/application_1693814926276_0001/container_1693814926276_0001_01_000001/pyspark.zip/pyspark/sql/pandas/conversion.py\", line 86, in toPandas\n",
      "    require_minimum_pandas_version()\n",
      "  File \"/mnt/yarn/usercache/livy/appcache/application_1693814926276_0001/container_1693814926276_0001_01_000001/pyspark.zip/pyspark/sql/pandas/utils.py\", line 40, in require_minimum_pandas_version\n",
      "    \"your version was %s.\" % (minimum_pandas_version, pandas.__version__)\n",
      "ImportError: Pandas >= 1.0.5 must be installed; however, your version was 0.25.1.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "splits = [ 0, 1, 3, 6, float('Inf')]\n",
    "inputCol  = \"years_since_submission_on_review_date\"\n",
    "outputCol = \"years_since_submission_on_review_date_bucket\"\n",
    "\n",
    "(interaction_level_df, submission_time_bucketizer, submission_time_pandas_df) = bucket_col_print_summary(df = interaction_level_df,\n",
    "                                                                              splits = splits,\n",
    "                                                                              inputCol  = inputCol,\n",
    "                                                                              outputCol = outputCol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2395619f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAoAAAAHgCAYAAAA10dzkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAAPYQAAD2EBqD+naQAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAIyUlEQVR4nO3WMQEAIAzAMMC/5+ECjiYKenbPzCwAADLO7wAAAN4ygAAAMQYQACDGAAIAxBhAAIAYAwgAEGMAAQBiDCAAQIwBBACIMYAAADEGEAAgxgACAMQYQACAGAMIABBjAAEAYgwgAECMAQQAiDGAAAAxBhAAIMYAAgDEGEAAgBgDCAAQYwABAGIMIABAjAEEAIgxgAAAMQYQACDGAAIAxBhAAIAYAwgAEGMAAQBiDCAAQIwBBACIMYAAADEGEAAgxgACAMQYQACAGAMIABBjAAEAYgwgAECMAQQAiDGAAAAxBhAAIMYAAgDEGEAAgBgDCAAQYwABAGIMIABAjAEEAIgxgAAAMQYQACDGAAIAxBhAAIAYAwgAEGMAAQBiDCAAQIwBBACIMYAAADEGEAAgxgACAMQYQACAGAMIABBjAAEAYgwgAECMAQQAiDGAAAAxBhAAIMYAAgDEGEAAgBgDCAAQYwABAGIMIABAjAEEAIgxgAAAMQYQACDGAAIAxBhAAIAYAwgAEGMAAQBiDCAAQIwBBACIMYAAADEGEAAgxgACAMQYQACAGAMIABBjAAEAYgwgAECMAQQAiDGAAAAxBhAAIMYAAgDEGEAAgBgDCAAQYwABAGIMIABAjAEEAIgxgAAAMQYQACDGAAIAxBhAAIAYAwgAEGMAAQBiDCAAQIwBBACIMYAAADEGEAAgxgACAMQYQACAGAMIABBjAAEAYgwgAECMAQQAiDGAAAAxBhAAIMYAAgDEGEAAgBgDCAAQYwABAGIMIABAjAEEAIgxgAAAMQYQACDGAAIAxBhAAIAYAwgAEGMAAQBiDCAAQIwBBACIMYAAADEGEAAgxgACAMQYQACAGAMIABBjAAEAYgwgAECMAQQAiDGAAAAxBhAAIMYAAgDEGEAAgBgDCAAQYwABAGIMIABAjAEEAIgxgAAAMQYQACDGAAIAxBhAAIAYAwgAEGMAAQBiDCAAQIwBBACIMYAAADEGEAAgxgACAMQYQACAGAMIABBjAAEAYgwgAECMAQQAiDGAAAAxBhAAIMYAAgDEGEAAgBgDCAAQYwABAGIMIABAjAEEAIgxgAAAMQYQACDGAAIAxBhAAIAYAwgAEGMAAQBiDCAAQIwBBACIMYAAADEGEAAgxgACAMQYQACAGAMIABBjAAEAYgwgAECMAQQAiDGAAAAxBhAAIMYAAgDEGEAAgBgDCAAQYwABAGIMIABAjAEEAIgxgAAAMQYQACDGAAIAxBhAAIAYAwgAEGMAAQBiDCAAQIwBBACIMYAAADEGEAAgxgACAMQYQACAGAMIABBjAAEAYgwgAECMAQQAiDGAAAAxBhAAIMYAAgDEGEAAgBgDCAAQYwABAGIMIABAjAEEAIgxgAAAMQYQACDGAAIAxBhAAIAYAwgAEGMAAQBiDCAAQIwBBACIMYAAADEGEAAgxgACAMQYQACAGAMIABBjAAEAYgwgAECMAQQAiDGAAAAxBhAAIMYAAgDEGEAAgBgDCAAQYwABAGIMIABAjAEEAIgxgAAAMQYQACDGAAIAxBhAAIAYAwgAEGMAAQBiDCAAQIwBBACIMYAAADEGEAAgxgACAMQYQACAGAMIABBjAAEAYgwgAECMAQQAiDGAAAAxBhAAIMYAAgDEGEAAgBgDCAAQYwABAGIMIABAjAEEAIgxgAAAMQYQACDGAAIAxBhAAIAYAwgAEGMAAQBiDCAAQIwBBACIMYAAADEGEAAgxgACAMQYQACAGAMIABBjAAEAYgwgAECMAQQAiDGAAAAxBhAAIMYAAgDEGEAAgBgDCAAQYwABAGIMIABAjAEEAIgxgAAAMQYQACDGAAIAxBhAAIAYAwgAEGMAAQBiDCAAQIwBBACIMYAAADEGEAAgxgACAMQYQACAGAMIABBjAAEAYgwgAECMAQQAiDGAAAAxBhAAIMYAAgDEGEAAgBgDCAAQYwABAGIMIABAjAEEAIgxgAAAMQYQACDGAAIAxBhAAIAYAwgAEGMAAQBiDCAAQIwBBACIMYAAADEGEAAgxgACAMQYQACAGAMIABBjAAEAYgwgAECMAQQAiDGAAAAxBhAAIMYAAgDEGEAAgBgDCAAQYwABAGIMIABAjAEEAIgxgAAAMQYQACDGAAIAxBhAAIAYAwgAEGMAAQBiDCAAQIwBBACIMYAAADEGEAAgxgACAMQYQACAGAMIABBjAAEAYgwgAECMAQQAiDGAAAAxBhAAIMYAAgDEGEAAgBgDCAAQYwABAGIMIABAjAEEAIgxgAAAMQYQACDGAAIAxBhAAIAYAwgAEGMAAQBiDCAAQIwBBACIMYAAADEGEAAgxgACAMQYQACAGAMIABBjAAEAYgwgAECMAQQAiDGAAAAxBhAAIMYAAgDEGEAAgBgDCAAQYwABAGIMIABAjAEEAIgxgAAAMQYQACDGAAIAxBhAAIAYAwgAEGMAAQBiDCAAQIwBBACIMYAAADEGEAAgxgACAMQYQACAGAMIABBjAAEAYgwgAECMAQQAiDGAAAAxBhAAIMYAAgDEGEAAgBgDCAAQYwABAGIMIABAjAEEAIgxgAAAMQYQACDGAAIAxBhAAIAYAwgAEGMAAQBiDCAAQIwBBACIMYAAADEGEAAgxgACAMQYQACAGAMIABBjAAEAYgwgAECMAQQAiDGAAAAxBhAAIMYAAgDEGEAAgBgDCAAQYwABAGIMIABAjAEEAIgxgAAAMQYQACDGAAIAxBhAAIAYAwgAEGMAAQBiDCAAQIwBBACIMYAAADEGEAAgxgACAMQYQACAGAMIABBjAAEAYgwgAECMAQQAiDGAAAAxBhAAIMYAAgDEGEAAgBgDCAAQYwABAGIMIABAjAEEAIgxgAAAMQYQACDGAAIAxBhAAIAYAwgAEGMAAQBiDCAAQIwBBACIMYAAADEGEAAgxgACAMQYQACAGAMIABBjAAEAYgwgAECMAQQAiDGAAAAxBhAAIMYAAgDEGEAAgBgDCAAQYwABAGIuJnkHvKensmIAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplot plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee68eb7",
   "metadata": {},
   "source": [
    "#### **2. `minutes`** \n",
    "\n",
    "[prep time]\n",
    "- Somewhat relevant\n",
    "- Low prep time is preferred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c2038a3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9668b67968d242739d3765d4c7bdadf4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0.0: 0, 0.01: 2.0, 0.05: 5.0, 0.25: 20.0, 0.5: 40.0, 0.75: 70.0, 0.95: 310.0, 0.99: 930.0, 1.0: 2147483647}"
     ]
    }
   ],
   "source": [
    "get_quantiles(df = interaction_level_df,\n",
    "              col_name = \"minutes\",\n",
    "              quantiles_list=[0.01, 0.05, 0.25, 0.5, 0.75, 0.95, 0.99])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "49e1b9d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "134cf2073a574ce29aa854ea26602343",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Capping prep time at 930 minutes\n",
    "\n",
    "interaction_level_df = (interaction_level_df\n",
    "                        .withColumn(\"minutes\",\n",
    "                                    F.when(interaction_level_df[\"minutes\"] > 930, 930)\n",
    "                                     .otherwise(interaction_level_df[\"minutes\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a9efcda9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a959600d8ff4439b8cdff868133a4d12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "An error occurred while calling o506.showString.\n",
      ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 2 in stage 54.0 failed 4 times, most recent failure: Lost task 2.3 in stage 54.0 (TID 174) (ip-10-0-5-13.ec2.internal executor 8): org.apache.spark.sql.execution.datasources.FileDownloadException: Failed to download file path: s3://demobucketpavi/foodreceipe/interaction_level_df/part-00022-e5684401-5b85-4096-8c1e-388e2578132c-c000.snappy.parquet, range: 0-15290018, partition values: [empty row], isDataPresent: false, eTag: 5ff5f14604f19e3cc59bf932822a4bb2-1\n",
      "\tat org.apache.spark.sql.execution.datasources.AsyncFileDownloader.next(AsyncFileDownloader.scala:142)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.getNextFile(FileScanRDD.scala:423)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:337)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:227)\n",
      "\tat org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:703)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithKeys_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:35)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hasNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:955)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.shuffle.sort.UnsafeShuffleWriter.write(UnsafeShuffleWriter.java:183)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:138)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1516)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "\tSuppressed: java.io.FileNotFoundException: No such file or directory 's3://demobucketpavi/foodreceipe/interaction_level_df/part-00015-e5684401-5b85-4096-8c1e-388e2578132c-c000.snappy.parquet'\n",
      "\t\tat com.amazon.ws.emr.hadoop.fs.s3n.S3NativeFileSystem.getFileStatus(S3NativeFileSystem.java:529)\n",
      "\t\tat com.amazon.ws.emr.hadoop.fs.EmrFileSystem.getFileStatus(EmrFileSystem.java:617)\n",
      "\t\tat org.apache.parquet.hadoop.util.HadoopInputFile.fromPath(HadoopInputFile.java:61)\n",
      "\t\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:39)\n",
      "\t\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat.$anonfun$buildPrefetcherWithPartitionValues$1(ParquetFileFormat.scala:537)\n",
      "\t\tat org.apache.spark.sql.execution.datasources.AsyncFileDownloader.org$apache$spark$sql$execution$datasources$AsyncFileDownloader$$downloadFile(AsyncFileDownloader.scala:93)\n",
      "\t\tat org.apache.spark.sql.execution.datasources.AsyncFileDownloader$$anon$1.call(AsyncFileDownloader.scala:73)\n",
      "\t\tat org.apache.spark.sql.execution.datasources.AsyncFileDownloader$$anon$1.call(AsyncFileDownloader.scala:72)\n",
      "\t\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n",
      "\t\t... 3 more\n",
      "Caused by: com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.AbortedException: \n",
      "\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleInterruptedException(AmazonHttpClient.java:880)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.execute(AmazonHttpClient.java:757)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.access$500(AmazonHttpClient.java:715)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutionBuilderImpl.execute(AmazonHttpClient.java:697)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:561)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:541)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5456)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5403)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.services.s3.AmazonS3Client.getObjectMetadata(AmazonS3Client.java:1372)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.s3.lite.call.GetObjectMetadataCall.perform(GetObjectMetadataCall.java:26)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.s3.lite.call.GetObjectMetadataCall.perform(GetObjectMetadataCall.java:12)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.s3.lite.executor.GlobalS3Executor$CallPerformer.call(GlobalS3Executor.java:111)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.s3.lite.executor.GlobalS3Executor.execute(GlobalS3Executor.java:138)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.s3.lite.AmazonS3LiteClient.invoke(AmazonS3LiteClient.java:191)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.s3.lite.AmazonS3LiteClient.invoke(AmazonS3LiteClient.java:186)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.s3.lite.AmazonS3LiteClient.getObjectMetadata(AmazonS3LiteClient.java:96)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.s3.lite.AbstractAmazonS3Lite.getObjectMetadata(AbstractAmazonS3Lite.java:43)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.s3n.Jets3tNativeFileSystemStore.getFileMetadataFromCacheOrS3(Jets3tNativeFileSystemStore.java:636)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.s3n.Jets3tNativeFileSystemStore.retrieveMetadata(Jets3tNativeFileSystemStore.java:320)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.s3n.Jets3tNativeFileSystemStore.isFolderUsingFolderObject(Jets3tNativeFileSystemStore.java:236)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.s3n.Jets3tNativeFileSystemStore.isFolder(Jets3tNativeFileSystemStore.java:212)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.s3n.S3NativeFileSystem.getFileStatus(S3NativeFileSystem.java:523)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.EmrFileSystem.getFileStatus(EmrFileSystem.java:617)\n",
      "\tat org.apache.parquet.hadoop.util.HadoopInputFile.fromPath(HadoopInputFile.java:61)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:39)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat.$anonfun$buildPrefetcherWithPartitionValues$1(ParquetFileFormat.scala:537)\n",
      "\tat org.apache.spark.sql.execution.datasources.AsyncFileDownloader.org$apache$spark$sql$execution$datasources$AsyncFileDownloader$$downloadFile(AsyncFileDownloader.scala:93)\n",
      "\tat org.apache.spark.sql.execution.datasources.AsyncFileDownloader$$anon$1.call(AsyncFileDownloader.scala:73)\n",
      "\tat org.apache.spark.sql.execution.datasources.AsyncFileDownloader$$anon$1.call(AsyncFileDownloader.scala:72)\n",
      "\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n",
      "\t... 3 more\n",
      "Caused by: com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.timers.client.SdkInterruptedException\n",
      "\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.checkInterrupted(AmazonHttpClient.java:935)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.afterAttempt(AmazonHttpClient.java:1085)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeHelper(AmazonHttpClient.java:1208)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.doExecute(AmazonHttpClient.java:814)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeWithTimer(AmazonHttpClient.java:781)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.execute(AmazonHttpClient.java:755)\n",
      "\t... 31 more\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2863)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2799)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2798)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2798)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1239)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1239)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1239)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3051)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2993)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveExecutor.checkNoFailures(AdaptiveExecutor.scala:154)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveExecutor.doRun(AdaptiveExecutor.scala:88)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveExecutor.tryRunningAndGetFuture(AdaptiveExecutor.scala:66)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveExecutor.execute(AdaptiveExecutor.scala:57)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$getFinalPhysicalPlan$1(AdaptiveSparkPlanExec.scala:249)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.getFinalPhysicalPlan(AdaptiveSparkPlanExec.scala:248)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:521)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.executeCollect(AdaptiveSparkPlanExec.scala:483)\n",
      "\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3932)\n",
      "\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2904)\n",
      "\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:3922)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:554)\n",
      "\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3920)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:107)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:224)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.executeQuery$1(SQLExecution.scala:114)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$7(SQLExecution.scala:139)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:107)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:224)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:139)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:245)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:138)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:68)\n",
      "\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3920)\n",
      "\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2904)\n",
      "\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3125)\n",
      "\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:290)\n",
      "\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:329)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.spark.sql.execution.datasources.FileDownloadException: Failed to download file path: s3://demobucketpavi/foodreceipe/interaction_level_df/part-00022-e5684401-5b85-4096-8c1e-388e2578132c-c000.snappy.parquet, range: 0-15290018, partition values: [empty row], isDataPresent: false, eTag: 5ff5f14604f19e3cc59bf932822a4bb2-1\n",
      "\tat org.apache.spark.sql.execution.datasources.AsyncFileDownloader.next(AsyncFileDownloader.scala:142)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.getNextFile(FileScanRDD.scala:423)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:337)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:227)\n",
      "\tat org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:703)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithKeys_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:35)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hasNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:955)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.shuffle.sort.UnsafeShuffleWriter.write(UnsafeShuffleWriter.java:183)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:138)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1516)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\t... 1 more\n",
      "\tSuppressed: java.io.FileNotFoundException: No such file or directory 's3://demobucketpavi/foodreceipe/interaction_level_df/part-00015-e5684401-5b85-4096-8c1e-388e2578132c-c000.snappy.parquet'\n",
      "\t\tat com.amazon.ws.emr.hadoop.fs.s3n.S3NativeFileSystem.getFileStatus(S3NativeFileSystem.java:529)\n",
      "\t\tat com.amazon.ws.emr.hadoop.fs.EmrFileSystem.getFileStatus(EmrFileSystem.java:617)\n",
      "\t\tat org.apache.parquet.hadoop.util.HadoopInputFile.fromPath(HadoopInputFile.java:61)\n",
      "\t\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:39)\n",
      "\t\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat.$anonfun$buildPrefetcherWithPartitionValues$1(ParquetFileFormat.scala:537)\n",
      "\t\tat org.apache.spark.sql.execution.datasources.AsyncFileDownloader.org$apache$spark$sql$execution$datasources$AsyncFileDownloader$$downloadFile(AsyncFileDownloader.scala:93)\n",
      "\t\tat org.apache.spark.sql.execution.datasources.AsyncFileDownloader$$anon$1.call(AsyncFileDownloader.scala:73)\n",
      "\t\tat org.apache.spark.sql.execution.datasources.AsyncFileDownloader$$anon$1.call(AsyncFileDownloader.scala:72)\n",
      "\t\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n",
      "\t\t... 3 more\n",
      "Caused by: com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.AbortedException: \n",
      "\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleInterruptedException(AmazonHttpClient.java:880)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.execute(AmazonHttpClient.java:757)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.access$500(AmazonHttpClient.java:715)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutionBuilderImpl.execute(AmazonHttpClient.java:697)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:561)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:541)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5456)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5403)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.services.s3.AmazonS3Client.getObjectMetadata(AmazonS3Client.java:1372)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.s3.lite.call.GetObjectMetadataCall.perform(GetObjectMetadataCall.java:26)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.s3.lite.call.GetObjectMetadataCall.perform(GetObjectMetadataCall.java:12)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.s3.lite.executor.GlobalS3Executor$CallPerformer.call(GlobalS3Executor.java:111)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.s3.lite.executor.GlobalS3Executor.execute(GlobalS3Executor.java:138)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.s3.lite.AmazonS3LiteClient.invoke(AmazonS3LiteClient.java:191)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.s3.lite.AmazonS3LiteClient.invoke(AmazonS3LiteClient.java:186)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.s3.lite.AmazonS3LiteClient.getObjectMetadata(AmazonS3LiteClient.java:96)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.s3.lite.AbstractAmazonS3Lite.getObjectMetadata(AbstractAmazonS3Lite.java:43)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.s3n.Jets3tNativeFileSystemStore.getFileMetadataFromCacheOrS3(Jets3tNativeFileSystemStore.java:636)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.s3n.Jets3tNativeFileSystemStore.retrieveMetadata(Jets3tNativeFileSystemStore.java:320)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.s3n.Jets3tNativeFileSystemStore.isFolderUsingFolderObject(Jets3tNativeFileSystemStore.java:236)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.s3n.Jets3tNativeFileSystemStore.isFolder(Jets3tNativeFileSystemStore.java:212)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.s3n.S3NativeFileSystem.getFileStatus(S3NativeFileSystem.java:523)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.EmrFileSystem.getFileStatus(EmrFileSystem.java:617)\n",
      "\tat org.apache.parquet.hadoop.util.HadoopInputFile.fromPath(HadoopInputFile.java:61)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:39)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat.$anonfun$buildPrefetcherWithPartitionValues$1(ParquetFileFormat.scala:537)\n",
      "\tat org.apache.spark.sql.execution.datasources.AsyncFileDownloader.org$apache$spark$sql$execution$datasources$AsyncFileDownloader$$downloadFile(AsyncFileDownloader.scala:93)\n",
      "\tat org.apache.spark.sql.execution.datasources.AsyncFileDownloader$$anon$1.call(AsyncFileDownloader.scala:73)\n",
      "\tat org.apache.spark.sql.execution.datasources.AsyncFileDownloader$$anon$1.call(AsyncFileDownloader.scala:72)\n",
      "\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n",
      "\t... 3 more\n",
      "Caused by: com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.timers.client.SdkInterruptedException\n",
      "\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.checkInterrupted(AmazonHttpClient.java:935)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.afterAttempt(AmazonHttpClient.java:1085)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeHelper(AmazonHttpClient.java:1208)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.doExecute(AmazonHttpClient.java:814)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeWithTimer(AmazonHttpClient.java:781)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.execute(AmazonHttpClient.java:755)\n",
      "\t... 31 more\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"<stdin>\", line 20, in get_column_distribution_summary\n",
      "  File \"/mnt/yarn/usercache/livy/appcache/application_1693814926276_0001/container_1693814926276_0001_01_000001/pyspark.zip/pyspark/sql/dataframe.py\", line 607, in show\n",
      "    print(self._jdf.showString(n, 20, vertical))\n",
      "  File \"/mnt/yarn/usercache/livy/appcache/application_1693814926276_0001/container_1693814926276_0001_01_000001/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\", line 1322, in __call__\n",
      "    answer, self.gateway_client, self.target_id, self.name)\n",
      "  File \"/mnt/yarn/usercache/livy/appcache/application_1693814926276_0001/container_1693814926276_0001_01_000001/pyspark.zip/pyspark/sql/utils.py\", line 190, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/mnt/yarn/usercache/livy/appcache/application_1693814926276_0001/container_1693814926276_0001_01_000001/py4j-0.10.9.5-src.zip/py4j/protocol.py\", line 328, in get_return_value\n",
      "    format(target_id, \".\", name), value)\n",
      "py4j.protocol.Py4JJavaError: An error occurred while calling o506.showString.\n",
      ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 2 in stage 54.0 failed 4 times, most recent failure: Lost task 2.3 in stage 54.0 (TID 174) (ip-10-0-5-13.ec2.internal executor 8): org.apache.spark.sql.execution.datasources.FileDownloadException: Failed to download file path: s3://demobucketpavi/foodreceipe/interaction_level_df/part-00022-e5684401-5b85-4096-8c1e-388e2578132c-c000.snappy.parquet, range: 0-15290018, partition values: [empty row], isDataPresent: false, eTag: 5ff5f14604f19e3cc59bf932822a4bb2-1\n",
      "\tat org.apache.spark.sql.execution.datasources.AsyncFileDownloader.next(AsyncFileDownloader.scala:142)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.getNextFile(FileScanRDD.scala:423)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:337)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:227)\n",
      "\tat org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:703)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithKeys_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:35)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hasNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:955)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.shuffle.sort.UnsafeShuffleWriter.write(UnsafeShuffleWriter.java:183)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:138)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1516)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "\tSuppressed: java.io.FileNotFoundException: No such file or directory 's3://demobucketpavi/foodreceipe/interaction_level_df/part-00015-e5684401-5b85-4096-8c1e-388e2578132c-c000.snappy.parquet'\n",
      "\t\tat com.amazon.ws.emr.hadoop.fs.s3n.S3NativeFileSystem.getFileStatus(S3NativeFileSystem.java:529)\n",
      "\t\tat com.amazon.ws.emr.hadoop.fs.EmrFileSystem.getFileStatus(EmrFileSystem.java:617)\n",
      "\t\tat org.apache.parquet.hadoop.util.HadoopInputFile.fromPath(HadoopInputFile.java:61)\n",
      "\t\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:39)\n",
      "\t\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat.$anonfun$buildPrefetcherWithPartitionValues$1(ParquetFileFormat.scala:537)\n",
      "\t\tat org.apache.spark.sql.execution.datasources.AsyncFileDownloader.org$apache$spark$sql$execution$datasources$AsyncFileDownloader$$downloadFile(AsyncFileDownloader.scala:93)\n",
      "\t\tat org.apache.spark.sql.execution.datasources.AsyncFileDownloader$$anon$1.call(AsyncFileDownloader.scala:73)\n",
      "\t\tat org.apache.spark.sql.execution.datasources.AsyncFileDownloader$$anon$1.call(AsyncFileDownloader.scala:72)\n",
      "\t\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n",
      "\t\t... 3 more\n",
      "Caused by: com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.AbortedException: \n",
      "\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleInterruptedException(AmazonHttpClient.java:880)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.execute(AmazonHttpClient.java:757)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.access$500(AmazonHttpClient.java:715)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutionBuilderImpl.execute(AmazonHttpClient.java:697)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:561)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:541)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5456)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5403)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.services.s3.AmazonS3Client.getObjectMetadata(AmazonS3Client.java:1372)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.s3.lite.call.GetObjectMetadataCall.perform(GetObjectMetadataCall.java:26)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.s3.lite.call.GetObjectMetadataCall.perform(GetObjectMetadataCall.java:12)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.s3.lite.executor.GlobalS3Executor$CallPerformer.call(GlobalS3Executor.java:111)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.s3.lite.executor.GlobalS3Executor.execute(GlobalS3Executor.java:138)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.s3.lite.AmazonS3LiteClient.invoke(AmazonS3LiteClient.java:191)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.s3.lite.AmazonS3LiteClient.invoke(AmazonS3LiteClient.java:186)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.s3.lite.AmazonS3LiteClient.getObjectMetadata(AmazonS3LiteClient.java:96)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.s3.lite.AbstractAmazonS3Lite.getObjectMetadata(AbstractAmazonS3Lite.java:43)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.s3n.Jets3tNativeFileSystemStore.getFileMetadataFromCacheOrS3(Jets3tNativeFileSystemStore.java:636)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.s3n.Jets3tNativeFileSystemStore.retrieveMetadata(Jets3tNativeFileSystemStore.java:320)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.s3n.Jets3tNativeFileSystemStore.isFolderUsingFolderObject(Jets3tNativeFileSystemStore.java:236)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.s3n.Jets3tNativeFileSystemStore.isFolder(Jets3tNativeFileSystemStore.java:212)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.s3n.S3NativeFileSystem.getFileStatus(S3NativeFileSystem.java:523)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.EmrFileSystem.getFileStatus(EmrFileSystem.java:617)\n",
      "\tat org.apache.parquet.hadoop.util.HadoopInputFile.fromPath(HadoopInputFile.java:61)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:39)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat.$anonfun$buildPrefetcherWithPartitionValues$1(ParquetFileFormat.scala:537)\n",
      "\tat org.apache.spark.sql.execution.datasources.AsyncFileDownloader.org$apache$spark$sql$execution$datasources$AsyncFileDownloader$$downloadFile(AsyncFileDownloader.scala:93)\n",
      "\tat org.apache.spark.sql.execution.datasources.AsyncFileDownloader$$anon$1.call(AsyncFileDownloader.scala:73)\n",
      "\tat org.apache.spark.sql.execution.datasources.AsyncFileDownloader$$anon$1.call(AsyncFileDownloader.scala:72)\n",
      "\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n",
      "\t... 3 more\n",
      "Caused by: com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.timers.client.SdkInterruptedException\n",
      "\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.checkInterrupted(AmazonHttpClient.java:935)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.afterAttempt(AmazonHttpClient.java:1085)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeHelper(AmazonHttpClient.java:1208)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.doExecute(AmazonHttpClient.java:814)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeWithTimer(AmazonHttpClient.java:781)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.execute(AmazonHttpClient.java:755)\n",
      "\t... 31 more\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2863)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2799)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2798)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2798)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1239)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1239)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1239)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3051)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2993)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveExecutor.checkNoFailures(AdaptiveExecutor.scala:154)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveExecutor.doRun(AdaptiveExecutor.scala:88)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveExecutor.tryRunningAndGetFuture(AdaptiveExecutor.scala:66)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveExecutor.execute(AdaptiveExecutor.scala:57)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$getFinalPhysicalPlan$1(AdaptiveSparkPlanExec.scala:249)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.getFinalPhysicalPlan(AdaptiveSparkPlanExec.scala:248)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:521)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.executeCollect(AdaptiveSparkPlanExec.scala:483)\n",
      "\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3932)\n",
      "\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2904)\n",
      "\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:3922)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:554)\n",
      "\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3920)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:107)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:224)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.executeQuery$1(SQLExecution.scala:114)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$7(SQLExecution.scala:139)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:107)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:224)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:139)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:245)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:138)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:68)\n",
      "\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3920)\n",
      "\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2904)\n",
      "\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3125)\n",
      "\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:290)\n",
      "\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:329)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.spark.sql.execution.datasources.FileDownloadException: Failed to download file path: s3://demobucketpavi/foodreceipe/interaction_level_df/part-00022-e5684401-5b85-4096-8c1e-388e2578132c-c000.snappy.parquet, range: 0-15290018, partition values: [empty row], isDataPresent: false, eTag: 5ff5f14604f19e3cc59bf932822a4bb2-1\n",
      "\tat org.apache.spark.sql.execution.datasources.AsyncFileDownloader.next(AsyncFileDownloader.scala:142)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.getNextFile(FileScanRDD.scala:423)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:337)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:227)\n",
      "\tat org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:703)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithKeys_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:35)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hasNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:955)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.shuffle.sort.UnsafeShuffleWriter.write(UnsafeShuffleWriter.java:183)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:138)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1516)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\t... 1 more\n",
      "\tSuppressed: java.io.FileNotFoundException: No such file or directory 's3://demobucketpavi/foodreceipe/interaction_level_df/part-00015-e5684401-5b85-4096-8c1e-388e2578132c-c000.snappy.parquet'\n",
      "\t\tat com.amazon.ws.emr.hadoop.fs.s3n.S3NativeFileSystem.getFileStatus(S3NativeFileSystem.java:529)\n",
      "\t\tat com.amazon.ws.emr.hadoop.fs.EmrFileSystem.getFileStatus(EmrFileSystem.java:617)\n",
      "\t\tat org.apache.parquet.hadoop.util.HadoopInputFile.fromPath(HadoopInputFile.java:61)\n",
      "\t\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:39)\n",
      "\t\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat.$anonfun$buildPrefetcherWithPartitionValues$1(ParquetFileFormat.scala:537)\n",
      "\t\tat org.apache.spark.sql.execution.datasources.AsyncFileDownloader.org$apache$spark$sql$execution$datasources$AsyncFileDownloader$$downloadFile(AsyncFileDownloader.scala:93)\n",
      "\t\tat org.apache.spark.sql.execution.datasources.AsyncFileDownloader$$anon$1.call(AsyncFileDownloader.scala:73)\n",
      "\t\tat org.apache.spark.sql.execution.datasources.AsyncFileDownloader$$anon$1.call(AsyncFileDownloader.scala:72)\n",
      "\t\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n",
      "\t\t... 3 more\n",
      "Caused by: com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.AbortedException: \n",
      "\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleInterruptedException(AmazonHttpClient.java:880)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.execute(AmazonHttpClient.java:757)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.access$500(AmazonHttpClient.java:715)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutionBuilderImpl.execute(AmazonHttpClient.java:697)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:561)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:541)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5456)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5403)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.services.s3.AmazonS3Client.getObjectMetadata(AmazonS3Client.java:1372)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.s3.lite.call.GetObjectMetadataCall.perform(GetObjectMetadataCall.java:26)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.s3.lite.call.GetObjectMetadataCall.perform(GetObjectMetadataCall.java:12)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.s3.lite.executor.GlobalS3Executor$CallPerformer.call(GlobalS3Executor.java:111)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.s3.lite.executor.GlobalS3Executor.execute(GlobalS3Executor.java:138)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.s3.lite.AmazonS3LiteClient.invoke(AmazonS3LiteClient.java:191)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.s3.lite.AmazonS3LiteClient.invoke(AmazonS3LiteClient.java:186)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.s3.lite.AmazonS3LiteClient.getObjectMetadata(AmazonS3LiteClient.java:96)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.s3.lite.AbstractAmazonS3Lite.getObjectMetadata(AbstractAmazonS3Lite.java:43)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.s3n.Jets3tNativeFileSystemStore.getFileMetadataFromCacheOrS3(Jets3tNativeFileSystemStore.java:636)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.s3n.Jets3tNativeFileSystemStore.retrieveMetadata(Jets3tNativeFileSystemStore.java:320)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.s3n.Jets3tNativeFileSystemStore.isFolderUsingFolderObject(Jets3tNativeFileSystemStore.java:236)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.s3n.Jets3tNativeFileSystemStore.isFolder(Jets3tNativeFileSystemStore.java:212)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.s3n.S3NativeFileSystem.getFileStatus(S3NativeFileSystem.java:523)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.EmrFileSystem.getFileStatus(EmrFileSystem.java:617)\n",
      "\tat org.apache.parquet.hadoop.util.HadoopInputFile.fromPath(HadoopInputFile.java:61)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:39)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat.$anonfun$buildPrefetcherWithPartitionValues$1(ParquetFileFormat.scala:537)\n",
      "\tat org.apache.spark.sql.execution.datasources.AsyncFileDownloader.org$apache$spark$sql$execution$datasources$AsyncFileDownloader$$downloadFile(AsyncFileDownloader.scala:93)\n",
      "\tat org.apache.spark.sql.execution.datasources.AsyncFileDownloader$$anon$1.call(AsyncFileDownloader.scala:73)\n",
      "\tat org.apache.spark.sql.execution.datasources.AsyncFileDownloader$$anon$1.call(AsyncFileDownloader.scala:72)\n",
      "\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n",
      "\t... 3 more\n",
      "Caused by: com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.timers.client.SdkInterruptedException\n",
      "\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.checkInterrupted(AmazonHttpClient.java:935)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.afterAttempt(AmazonHttpClient.java:1085)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeHelper(AmazonHttpClient.java:1208)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.doExecute(AmazonHttpClient.java:814)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeWithTimer(AmazonHttpClient.java:781)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.execute(AmazonHttpClient.java:755)\n",
      "\t... 31 more\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# investigating recipes with minutes = 0 -> Look at n_steps for such recipes.\n",
    "\n",
    "get_column_distribution_summary(df = (interaction_level_df\n",
    "                                      .filter('minutes == 0')\n",
    "                                      .withColumn('n_steps_modified', (F.when(interaction_level_df['n_steps'] >= 10, \">= 10\")\n",
    "                                                                        .otherwise(F.lpad(interaction_level_df['n_steps'],2,\"0\"))))),\n",
    "                                col_name = 'n_steps_modified')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "509cf653",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a035b843f624490914dae58095694b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[user_id: int, recipe_id: int, rating: int, review: string, review_date: date, name: string, minutes: int, contributor_id: int, submitted: date, tags: array<string>, nutrition: string, n_steps: int, steps: string, description: string, ingredients: string, n_ingredients: int, calories: float, total_fat_PDV: float, sugar_PDV: float, sodium_PDV: float, protein_PDV: float, saturated_fat_PDV: float, carbohydrates_PDV: float, calories_per_100_cal: double, total_fat_per_100_cal: double, sugar_per_100_cal: double, sodium_per_100_cal: double, protein_per_100_cal: double, saturated_fat_per_100_cal: double, carbohydrates_per_100_cal: double, days_since_submission_on_review_date: int, months_since_submission_on_review_date: double, years_since_submission_on_review_date: double]"
     ]
    }
   ],
   "source": [
    "# let's look at some examples with 1 step only to see if this makes sense\n",
    "\n",
    "interaction_level_df.filter('minutes == 0 and n_steps == 1')\n",
    "#.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "bd0656d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c3894e379524507a0d23b9227dcda7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "An error occurred while calling o469.collectToPython.\n",
      ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 53.0 failed 4 times, most recent failure: Lost task 1.3 in stage 53.0 (TID 154) (ip-10-0-5-13.ec2.internal executor 7): org.apache.spark.sql.execution.datasources.FileDownloadException: Failed to download file path: s3://demobucketpavi/foodreceipe/interaction_level_df/part-00012-e5684401-5b85-4096-8c1e-388e2578132c-c000.snappy.parquet, range: 0-15370992, partition values: [empty row], isDataPresent: false, eTag: 41cdd51181a98dfc482a4f2611fc2e96-1\n",
      "\tat org.apache.spark.sql.execution.datasources.AsyncFileDownloader.next(AsyncFileDownloader.scala:142)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.getNextFile(FileScanRDD.scala:423)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:337)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:227)\n",
      "\tat org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:703)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithKeys_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:35)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hasNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:955)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.shuffle.sort.UnsafeShuffleWriter.write(UnsafeShuffleWriter.java:183)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:138)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1516)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: java.io.FileNotFoundException: No such file or directory 's3://demobucketpavi/foodreceipe/interaction_level_df/part-00012-e5684401-5b85-4096-8c1e-388e2578132c-c000.snappy.parquet'\n",
      "\tat com.amazon.ws.emr.hadoop.fs.s3n.S3NativeFileSystem.getFileStatus(S3NativeFileSystem.java:529)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.EmrFileSystem.getFileStatus(EmrFileSystem.java:617)\n",
      "\tat org.apache.parquet.hadoop.util.HadoopInputFile.fromPath(HadoopInputFile.java:61)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:39)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat.$anonfun$buildPrefetcherWithPartitionValues$1(ParquetFileFormat.scala:537)\n",
      "\tat org.apache.spark.sql.execution.datasources.AsyncFileDownloader.org$apache$spark$sql$execution$datasources$AsyncFileDownloader$$downloadFile(AsyncFileDownloader.scala:93)\n",
      "\tat org.apache.spark.sql.execution.datasources.AsyncFileDownloader$$anon$1.call(AsyncFileDownloader.scala:73)\n",
      "\tat org.apache.spark.sql.execution.datasources.AsyncFileDownloader$$anon$1.call(AsyncFileDownloader.scala:72)\n",
      "\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n",
      "\t... 3 more\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2863)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2799)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2798)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2798)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1239)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1239)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1239)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3051)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2993)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveExecutor.checkNoFailures(AdaptiveExecutor.scala:154)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveExecutor.doRun(AdaptiveExecutor.scala:88)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveExecutor.tryRunningAndGetFuture(AdaptiveExecutor.scala:66)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveExecutor.execute(AdaptiveExecutor.scala:57)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$getFinalPhysicalPlan$1(AdaptiveSparkPlanExec.scala:249)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.getFinalPhysicalPlan(AdaptiveSparkPlanExec.scala:248)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:521)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.executeCollect(AdaptiveSparkPlanExec.scala:483)\n",
      "\tat org.apache.spark.sql.Dataset.$anonfun$collectToPython$1(Dataset.scala:3756)\n",
      "\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:3922)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:554)\n",
      "\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3920)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:107)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:224)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.executeQuery$1(SQLExecution.scala:114)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$7(SQLExecution.scala:139)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:107)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:224)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:139)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:245)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:138)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:68)\n",
      "\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3920)\n",
      "\tat org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:3753)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.spark.sql.execution.datasources.FileDownloadException: Failed to download file path: s3://demobucketpavi/foodreceipe/interaction_level_df/part-00012-e5684401-5b85-4096-8c1e-388e2578132c-c000.snappy.parquet, range: 0-15370992, partition values: [empty row], isDataPresent: false, eTag: 41cdd51181a98dfc482a4f2611fc2e96-1\n",
      "\tat org.apache.spark.sql.execution.datasources.AsyncFileDownloader.next(AsyncFileDownloader.scala:142)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.getNextFile(FileScanRDD.scala:423)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:337)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:227)\n",
      "\tat org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:703)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithKeys_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:35)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hasNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:955)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.shuffle.sort.UnsafeShuffleWriter.write(UnsafeShuffleWriter.java:183)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:138)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1516)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\t... 1 more\n",
      "Caused by: java.io.FileNotFoundException: No such file or directory 's3://demobucketpavi/foodreceipe/interaction_level_df/part-00012-e5684401-5b85-4096-8c1e-388e2578132c-c000.snappy.parquet'\n",
      "\tat com.amazon.ws.emr.hadoop.fs.s3n.S3NativeFileSystem.getFileStatus(S3NativeFileSystem.java:529)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.EmrFileSystem.getFileStatus(EmrFileSystem.java:617)\n",
      "\tat org.apache.parquet.hadoop.util.HadoopInputFile.fromPath(HadoopInputFile.java:61)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:39)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat.$anonfun$buildPrefetcherWithPartitionValues$1(ParquetFileFormat.scala:537)\n",
      "\tat org.apache.spark.sql.execution.datasources.AsyncFileDownloader.org$apache$spark$sql$execution$datasources$AsyncFileDownloader$$downloadFile(AsyncFileDownloader.scala:93)\n",
      "\tat org.apache.spark.sql.execution.datasources.AsyncFileDownloader$$anon$1.call(AsyncFileDownloader.scala:73)\n",
      "\tat org.apache.spark.sql.execution.datasources.AsyncFileDownloader$$anon$1.call(AsyncFileDownloader.scala:72)\n",
      "\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n",
      "\t... 3 more\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"<stdin>\", line 21, in get_n_items_satisfying_condition\n",
      "  File \"/mnt/yarn/usercache/livy/appcache/application_1693814926276_0001/container_1693814926276_0001_01_000001/pyspark.zip/pyspark/sql/dataframe.py\", line 1939, in first\n",
      "    return self.head()\n",
      "  File \"/mnt/yarn/usercache/livy/appcache/application_1693814926276_0001/container_1693814926276_0001_01_000001/pyspark.zip/pyspark/sql/dataframe.py\", line 1925, in head\n",
      "    rs = self.head(1)\n",
      "  File \"/mnt/yarn/usercache/livy/appcache/application_1693814926276_0001/container_1693814926276_0001_01_000001/pyspark.zip/pyspark/sql/dataframe.py\", line 1927, in head\n",
      "    return self.take(n)\n",
      "  File \"/mnt/yarn/usercache/livy/appcache/application_1693814926276_0001/container_1693814926276_0001_01_000001/pyspark.zip/pyspark/sql/dataframe.py\", line 869, in take\n",
      "    return self.limit(num).collect()\n",
      "  File \"/mnt/yarn/usercache/livy/appcache/application_1693814926276_0001/container_1693814926276_0001_01_000001/pyspark.zip/pyspark/sql/dataframe.py\", line 818, in collect\n",
      "    sock_info = self._jdf.collectToPython()\n",
      "  File \"/mnt/yarn/usercache/livy/appcache/application_1693814926276_0001/container_1693814926276_0001_01_000001/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\", line 1322, in __call__\n",
      "    answer, self.gateway_client, self.target_id, self.name)\n",
      "  File \"/mnt/yarn/usercache/livy/appcache/application_1693814926276_0001/container_1693814926276_0001_01_000001/pyspark.zip/pyspark/sql/utils.py\", line 190, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/mnt/yarn/usercache/livy/appcache/application_1693814926276_0001/container_1693814926276_0001_01_000001/py4j-0.10.9.5-src.zip/py4j/protocol.py\", line 328, in get_return_value\n",
      "    format(target_id, \".\", name), value)\n",
      "py4j.protocol.Py4JJavaError: An error occurred while calling o469.collectToPython.\n",
      ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 53.0 failed 4 times, most recent failure: Lost task 1.3 in stage 53.0 (TID 154) (ip-10-0-5-13.ec2.internal executor 7): org.apache.spark.sql.execution.datasources.FileDownloadException: Failed to download file path: s3://demobucketpavi/foodreceipe/interaction_level_df/part-00012-e5684401-5b85-4096-8c1e-388e2578132c-c000.snappy.parquet, range: 0-15370992, partition values: [empty row], isDataPresent: false, eTag: 41cdd51181a98dfc482a4f2611fc2e96-1\n",
      "\tat org.apache.spark.sql.execution.datasources.AsyncFileDownloader.next(AsyncFileDownloader.scala:142)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.getNextFile(FileScanRDD.scala:423)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:337)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:227)\n",
      "\tat org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:703)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithKeys_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:35)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hasNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:955)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.shuffle.sort.UnsafeShuffleWriter.write(UnsafeShuffleWriter.java:183)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:138)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1516)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: java.io.FileNotFoundException: No such file or directory 's3://demobucketpavi/foodreceipe/interaction_level_df/part-00012-e5684401-5b85-4096-8c1e-388e2578132c-c000.snappy.parquet'\n",
      "\tat com.amazon.ws.emr.hadoop.fs.s3n.S3NativeFileSystem.getFileStatus(S3NativeFileSystem.java:529)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.EmrFileSystem.getFileStatus(EmrFileSystem.java:617)\n",
      "\tat org.apache.parquet.hadoop.util.HadoopInputFile.fromPath(HadoopInputFile.java:61)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:39)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat.$anonfun$buildPrefetcherWithPartitionValues$1(ParquetFileFormat.scala:537)\n",
      "\tat org.apache.spark.sql.execution.datasources.AsyncFileDownloader.org$apache$spark$sql$execution$datasources$AsyncFileDownloader$$downloadFile(AsyncFileDownloader.scala:93)\n",
      "\tat org.apache.spark.sql.execution.datasources.AsyncFileDownloader$$anon$1.call(AsyncFileDownloader.scala:73)\n",
      "\tat org.apache.spark.sql.execution.datasources.AsyncFileDownloader$$anon$1.call(AsyncFileDownloader.scala:72)\n",
      "\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n",
      "\t... 3 more\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2863)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2799)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2798)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2798)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1239)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1239)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1239)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3051)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2993)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveExecutor.checkNoFailures(AdaptiveExecutor.scala:154)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveExecutor.doRun(AdaptiveExecutor.scala:88)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveExecutor.tryRunningAndGetFuture(AdaptiveExecutor.scala:66)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveExecutor.execute(AdaptiveExecutor.scala:57)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$getFinalPhysicalPlan$1(AdaptiveSparkPlanExec.scala:249)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.getFinalPhysicalPlan(AdaptiveSparkPlanExec.scala:248)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:521)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.executeCollect(AdaptiveSparkPlanExec.scala:483)\n",
      "\tat org.apache.spark.sql.Dataset.$anonfun$collectToPython$1(Dataset.scala:3756)\n",
      "\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:3922)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:554)\n",
      "\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3920)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:107)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:224)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.executeQuery$1(SQLExecution.scala:114)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$7(SQLExecution.scala:139)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:107)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:224)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:139)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:245)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:138)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:68)\n",
      "\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3920)\n",
      "\tat org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:3753)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.spark.sql.execution.datasources.FileDownloadException: Failed to download file path: s3://demobucketpavi/foodreceipe/interaction_level_df/part-00012-e5684401-5b85-4096-8c1e-388e2578132c-c000.snappy.parquet, range: 0-15370992, partition values: [empty row], isDataPresent: false, eTag: 41cdd51181a98dfc482a4f2611fc2e96-1\n",
      "\tat org.apache.spark.sql.execution.datasources.AsyncFileDownloader.next(AsyncFileDownloader.scala:142)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.getNextFile(FileScanRDD.scala:423)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:337)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:227)\n",
      "\tat org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:703)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithKeys_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:35)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hasNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:955)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.shuffle.sort.UnsafeShuffleWriter.write(UnsafeShuffleWriter.java:183)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:138)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1516)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\t... 1 more\n",
      "Caused by: java.io.FileNotFoundException: No such file or directory 's3://demobucketpavi/foodreceipe/interaction_level_df/part-00012-e5684401-5b85-4096-8c1e-388e2578132c-c000.snappy.parquet'\n",
      "\tat com.amazon.ws.emr.hadoop.fs.s3n.S3NativeFileSystem.getFileStatus(S3NativeFileSystem.java:529)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.EmrFileSystem.getFileStatus(EmrFileSystem.java:617)\n",
      "\tat org.apache.parquet.hadoop.util.HadoopInputFile.fromPath(HadoopInputFile.java:61)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:39)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat.$anonfun$buildPrefetcherWithPartitionValues$1(ParquetFileFormat.scala:537)\n",
      "\tat org.apache.spark.sql.execution.datasources.AsyncFileDownloader.org$apache$spark$sql$execution$datasources$AsyncFileDownloader$$downloadFile(AsyncFileDownloader.scala:93)\n",
      "\tat org.apache.spark.sql.execution.datasources.AsyncFileDownloader$$anon$1.call(AsyncFileDownloader.scala:73)\n",
      "\tat org.apache.spark.sql.execution.datasources.AsyncFileDownloader$$anon$1.call(AsyncFileDownloader.scala:72)\n",
      "\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n",
      "\t... 3 more\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "get_n_items_satisfying_condition(df = interaction_level_df,\n",
    "                                 condition = 'minutes == 0',\n",
    "                                 aggregation_level = \"recipe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "03e3fd59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2041ad5ff53a491fa8e3ea8187ec29eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Remove recipes with cook time zero\n",
    "\n",
    "interaction_level_df = interaction_level_df.filter(\"minutes > 0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "603439c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e7387651df14eeb8f6e750a1fb7ae58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "Pandas >= 1.0.5 must be installed; however, your version was 0.25.1.\n",
      "Traceback (most recent call last):\n",
      "  File \"<stdin>\", line 43, in bucket_col_print_summary\n",
      "  File \"/mnt/yarn/usercache/livy/appcache/application_1693814926276_0001/container_1693814926276_0001_01_000001/pyspark.zip/pyspark/sql/pandas/conversion.py\", line 86, in toPandas\n",
      "    require_minimum_pandas_version()\n",
      "  File \"/mnt/yarn/usercache/livy/appcache/application_1693814926276_0001/container_1693814926276_0001_01_000001/pyspark.zip/pyspark/sql/pandas/utils.py\", line 40, in require_minimum_pandas_version\n",
      "    \"your version was %s.\" % (minimum_pandas_version, pandas.__version__)\n",
      "ImportError: Pandas >= 1.0.5 must be installed; however, your version was 0.25.1.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "splits = [0, 5, 15, 30, 60, 300, 900, float('Inf')]\n",
    "inputCol  = \"minutes\"\n",
    "outputCol = \"prep_time_bucket\"\n",
    "\n",
    "(interaction_level_df, prep_time_bucketizer, prep_time_summary_pandas_df) = bucket_col_print_summary(df = interaction_level_df,\n",
    "                                                                              splits = splits,\n",
    "                                                                              inputCol  = inputCol,\n",
    "                                                                              outputCol = outputCol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5d040184",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplot plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3cb6116",
   "metadata": {},
   "source": [
    "**3. `n_steps`**\n",
    "\n",
    "- Clearly relevant\n",
    "- Recipes with less than 2 steps are rated high\n",
    "- Recipes with more than 29 steps are rated very low"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "653f7324",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "065dfe089bb140c9ba443dbcb95176a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "An error occurred while calling o562.collectToPython.\n",
      ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 3 in stage 55.0 failed 4 times, most recent failure: Lost task 3.3 in stage 55.0 (TID 190) (ip-10-0-5-13.ec2.internal executor 9): org.apache.spark.sql.execution.datasources.FileDownloadException: Failed to download file path: s3://demobucketpavi/foodreceipe/interaction_level_df/part-00000-e5684401-5b85-4096-8c1e-388e2578132c-c000.snappy.parquet, range: 0-15170243, partition values: [empty row], isDataPresent: false, eTag: 20837f8b9d002c38bc646108423ff32f-1\n",
      "\tat org.apache.spark.sql.execution.datasources.AsyncFileDownloader.next(AsyncFileDownloader.scala:142)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.getNextFile(FileScanRDD.scala:423)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:337)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:227)\n",
      "\tat org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:703)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithoutKey_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:35)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hasNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:955)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:142)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:138)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1516)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: java.io.FileNotFoundException: No such file or directory 's3://demobucketpavi/foodreceipe/interaction_level_df/part-00000-e5684401-5b85-4096-8c1e-388e2578132c-c000.snappy.parquet'\n",
      "\tat com.amazon.ws.emr.hadoop.fs.s3n.S3NativeFileSystem.getFileStatus(S3NativeFileSystem.java:529)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.EmrFileSystem.getFileStatus(EmrFileSystem.java:617)\n",
      "\tat org.apache.parquet.hadoop.util.HadoopInputFile.fromPath(HadoopInputFile.java:61)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:39)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat.$anonfun$buildPrefetcherWithPartitionValues$1(ParquetFileFormat.scala:537)\n",
      "\tat org.apache.spark.sql.execution.datasources.AsyncFileDownloader.org$apache$spark$sql$execution$datasources$AsyncFileDownloader$$downloadFile(AsyncFileDownloader.scala:93)\n",
      "\tat org.apache.spark.sql.execution.datasources.AsyncFileDownloader$$anon$1.call(AsyncFileDownloader.scala:73)\n",
      "\tat org.apache.spark.sql.execution.datasources.AsyncFileDownloader$$anon$1.call(AsyncFileDownloader.scala:72)\n",
      "\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n",
      "\t... 3 more\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2863)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2799)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2798)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2798)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1239)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1239)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1239)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3051)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2993)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveExecutor.checkNoFailures(AdaptiveExecutor.scala:154)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveExecutor.doRun(AdaptiveExecutor.scala:88)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveExecutor.tryRunningAndGetFuture(AdaptiveExecutor.scala:66)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveExecutor.execute(AdaptiveExecutor.scala:57)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$getFinalPhysicalPlan$1(AdaptiveSparkPlanExec.scala:249)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.getFinalPhysicalPlan(AdaptiveSparkPlanExec.scala:248)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:521)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.executeCollect(AdaptiveSparkPlanExec.scala:483)\n",
      "\tat org.apache.spark.sql.Dataset.$anonfun$collectToPython$1(Dataset.scala:3756)\n",
      "\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:3922)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:554)\n",
      "\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3920)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:107)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:224)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.executeQuery$1(SQLExecution.scala:114)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$7(SQLExecution.scala:139)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:107)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:224)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:139)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:245)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:138)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:68)\n",
      "\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3920)\n",
      "\tat org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:3753)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.spark.sql.execution.datasources.FileDownloadException: Failed to download file path: s3://demobucketpavi/foodreceipe/interaction_level_df/part-00000-e5684401-5b85-4096-8c1e-388e2578132c-c000.snappy.parquet, range: 0-15170243, partition values: [empty row], isDataPresent: false, eTag: 20837f8b9d002c38bc646108423ff32f-1\n",
      "\tat org.apache.spark.sql.execution.datasources.AsyncFileDownloader.next(AsyncFileDownloader.scala:142)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.getNextFile(FileScanRDD.scala:423)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:337)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:227)\n",
      "\tat org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:703)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithoutKey_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:35)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hasNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:955)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:142)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:138)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1516)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\t... 1 more\n",
      "Caused by: java.io.FileNotFoundException: No such file or directory 's3://demobucketpavi/foodreceipe/interaction_level_df/part-00000-e5684401-5b85-4096-8c1e-388e2578132c-c000.snappy.parquet'\n",
      "\tat com.amazon.ws.emr.hadoop.fs.s3n.S3NativeFileSystem.getFileStatus(S3NativeFileSystem.java:529)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.EmrFileSystem.getFileStatus(EmrFileSystem.java:617)\n",
      "\tat org.apache.parquet.hadoop.util.HadoopInputFile.fromPath(HadoopInputFile.java:61)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:39)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat.$anonfun$buildPrefetcherWithPartitionValues$1(ParquetFileFormat.scala:537)\n",
      "\tat org.apache.spark.sql.execution.datasources.AsyncFileDownloader.org$apache$spark$sql$execution$datasources$AsyncFileDownloader$$downloadFile(AsyncFileDownloader.scala:93)\n",
      "\tat org.apache.spark.sql.execution.datasources.AsyncFileDownloader$$anon$1.call(AsyncFileDownloader.scala:73)\n",
      "\tat org.apache.spark.sql.execution.datasources.AsyncFileDownloader$$anon$1.call(AsyncFileDownloader.scala:72)\n",
      "\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n",
      "\t... 3 more\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"<stdin>\", line 14, in get_quantiles\n",
      "  File \"/mnt/yarn/usercache/livy/appcache/application_1693814926276_0001/container_1693814926276_0001_01_000001/pyspark.zip/pyspark/sql/dataframe.py\", line 1939, in first\n",
      "    return self.head()\n",
      "  File \"/mnt/yarn/usercache/livy/appcache/application_1693814926276_0001/container_1693814926276_0001_01_000001/pyspark.zip/pyspark/sql/dataframe.py\", line 1925, in head\n",
      "    rs = self.head(1)\n",
      "  File \"/mnt/yarn/usercache/livy/appcache/application_1693814926276_0001/container_1693814926276_0001_01_000001/pyspark.zip/pyspark/sql/dataframe.py\", line 1927, in head\n",
      "    return self.take(n)\n",
      "  File \"/mnt/yarn/usercache/livy/appcache/application_1693814926276_0001/container_1693814926276_0001_01_000001/pyspark.zip/pyspark/sql/dataframe.py\", line 869, in take\n",
      "    return self.limit(num).collect()\n",
      "  File \"/mnt/yarn/usercache/livy/appcache/application_1693814926276_0001/container_1693814926276_0001_01_000001/pyspark.zip/pyspark/sql/dataframe.py\", line 818, in collect\n",
      "    sock_info = self._jdf.collectToPython()\n",
      "  File \"/mnt/yarn/usercache/livy/appcache/application_1693814926276_0001/container_1693814926276_0001_01_000001/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\", line 1322, in __call__\n",
      "    answer, self.gateway_client, self.target_id, self.name)\n",
      "  File \"/mnt/yarn/usercache/livy/appcache/application_1693814926276_0001/container_1693814926276_0001_01_000001/pyspark.zip/pyspark/sql/utils.py\", line 190, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/mnt/yarn/usercache/livy/appcache/application_1693814926276_0001/container_1693814926276_0001_01_000001/py4j-0.10.9.5-src.zip/py4j/protocol.py\", line 328, in get_return_value\n",
      "    format(target_id, \".\", name), value)\n",
      "py4j.protocol.Py4JJavaError: An error occurred while calling o562.collectToPython.\n",
      ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 3 in stage 55.0 failed 4 times, most recent failure: Lost task 3.3 in stage 55.0 (TID 190) (ip-10-0-5-13.ec2.internal executor 9): org.apache.spark.sql.execution.datasources.FileDownloadException: Failed to download file path: s3://demobucketpavi/foodreceipe/interaction_level_df/part-00000-e5684401-5b85-4096-8c1e-388e2578132c-c000.snappy.parquet, range: 0-15170243, partition values: [empty row], isDataPresent: false, eTag: 20837f8b9d002c38bc646108423ff32f-1\n",
      "\tat org.apache.spark.sql.execution.datasources.AsyncFileDownloader.next(AsyncFileDownloader.scala:142)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.getNextFile(FileScanRDD.scala:423)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:337)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:227)\n",
      "\tat org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:703)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithoutKey_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:35)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hasNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:955)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:142)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:138)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1516)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: java.io.FileNotFoundException: No such file or directory 's3://demobucketpavi/foodreceipe/interaction_level_df/part-00000-e5684401-5b85-4096-8c1e-388e2578132c-c000.snappy.parquet'\n",
      "\tat com.amazon.ws.emr.hadoop.fs.s3n.S3NativeFileSystem.getFileStatus(S3NativeFileSystem.java:529)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.EmrFileSystem.getFileStatus(EmrFileSystem.java:617)\n",
      "\tat org.apache.parquet.hadoop.util.HadoopInputFile.fromPath(HadoopInputFile.java:61)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:39)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat.$anonfun$buildPrefetcherWithPartitionValues$1(ParquetFileFormat.scala:537)\n",
      "\tat org.apache.spark.sql.execution.datasources.AsyncFileDownloader.org$apache$spark$sql$execution$datasources$AsyncFileDownloader$$downloadFile(AsyncFileDownloader.scala:93)\n",
      "\tat org.apache.spark.sql.execution.datasources.AsyncFileDownloader$$anon$1.call(AsyncFileDownloader.scala:73)\n",
      "\tat org.apache.spark.sql.execution.datasources.AsyncFileDownloader$$anon$1.call(AsyncFileDownloader.scala:72)\n",
      "\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n",
      "\t... 3 more\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2863)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2799)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2798)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2798)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1239)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1239)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1239)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3051)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2993)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveExecutor.checkNoFailures(AdaptiveExecutor.scala:154)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveExecutor.doRun(AdaptiveExecutor.scala:88)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveExecutor.tryRunningAndGetFuture(AdaptiveExecutor.scala:66)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveExecutor.execute(AdaptiveExecutor.scala:57)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$getFinalPhysicalPlan$1(AdaptiveSparkPlanExec.scala:249)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.getFinalPhysicalPlan(AdaptiveSparkPlanExec.scala:248)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:521)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.executeCollect(AdaptiveSparkPlanExec.scala:483)\n",
      "\tat org.apache.spark.sql.Dataset.$anonfun$collectToPython$1(Dataset.scala:3756)\n",
      "\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:3922)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:554)\n",
      "\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3920)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:107)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:224)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.executeQuery$1(SQLExecution.scala:114)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$7(SQLExecution.scala:139)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:107)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:224)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:139)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:245)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:138)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:68)\n",
      "\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3920)\n",
      "\tat org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:3753)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.spark.sql.execution.datasources.FileDownloadException: Failed to download file path: s3://demobucketpavi/foodreceipe/interaction_level_df/part-00000-e5684401-5b85-4096-8c1e-388e2578132c-c000.snappy.parquet, range: 0-15170243, partition values: [empty row], isDataPresent: false, eTag: 20837f8b9d002c38bc646108423ff32f-1\n",
      "\tat org.apache.spark.sql.execution.datasources.AsyncFileDownloader.next(AsyncFileDownloader.scala:142)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.getNextFile(FileScanRDD.scala:423)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:337)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:227)\n",
      "\tat org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:703)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithoutKey_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:35)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hasNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:955)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:142)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:138)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1516)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\t... 1 more\n",
      "Caused by: java.io.FileNotFoundException: No such file or directory 's3://demobucketpavi/foodreceipe/interaction_level_df/part-00000-e5684401-5b85-4096-8c1e-388e2578132c-c000.snappy.parquet'\n",
      "\tat com.amazon.ws.emr.hadoop.fs.s3n.S3NativeFileSystem.getFileStatus(S3NativeFileSystem.java:529)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.EmrFileSystem.getFileStatus(EmrFileSystem.java:617)\n",
      "\tat org.apache.parquet.hadoop.util.HadoopInputFile.fromPath(HadoopInputFile.java:61)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:39)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat.$anonfun$buildPrefetcherWithPartitionValues$1(ParquetFileFormat.scala:537)\n",
      "\tat org.apache.spark.sql.execution.datasources.AsyncFileDownloader.org$apache$spark$sql$execution$datasources$AsyncFileDownloader$$downloadFile(AsyncFileDownloader.scala:93)\n",
      "\tat org.apache.spark.sql.execution.datasources.AsyncFileDownloader$$anon$1.call(AsyncFileDownloader.scala:73)\n",
      "\tat org.apache.spark.sql.execution.datasources.AsyncFileDownloader$$anon$1.call(AsyncFileDownloader.scala:72)\n",
      "\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n",
      "\t... 3 more\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "get_quantiles(df = interaction_level_df,\n",
    "              col_name = \"n_steps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2f4acf93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd99ecc1098e4e1e8c90beffebc36b25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "An error occurred while calling o566.showString.\n",
      ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 56.0 failed 4 times, most recent failure: Lost task 0.3 in stage 56.0 (TID 197) (ip-10-0-5-13.ec2.internal executor 9): java.io.FileNotFoundException: \n",
      "No such file or directory 's3://demobucketpavi/foodreceipe/interaction_level_df/part-00005-e5684401-5b85-4096-8c1e-388e2578132c-c000.snappy.parquet'\n",
      "\n",
      "It is possible the underlying files have been updated. You can explicitly invalidate\n",
      "the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by\n",
      "recreating the Dataset/DataFrame involved.\n",
      "       \n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:661)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:330)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:389)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:227)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:35)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hasNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:955)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:383)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:138)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1516)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2863)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2799)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2798)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2798)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1239)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1239)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1239)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3051)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2993)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1009)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2229)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2250)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2269)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:533)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:486)\n",
      "\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:48)\n",
      "\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3932)\n",
      "\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2904)\n",
      "\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:3922)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:554)\n",
      "\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3920)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:107)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:224)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.executeQuery$1(SQLExecution.scala:114)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$7(SQLExecution.scala:139)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:107)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:224)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:139)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:245)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:138)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:68)\n",
      "\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3920)\n",
      "\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2904)\n",
      "\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3125)\n",
      "\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:290)\n",
      "\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:329)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: java.io.FileNotFoundException: \n",
      "No such file or directory 's3://demobucketpavi/foodreceipe/interaction_level_df/part-00005-e5684401-5b85-4096-8c1e-388e2578132c-c000.snappy.parquet'\n",
      "\n",
      "It is possible the underlying files have been updated. You can explicitly invalidate\n",
      "the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by\n",
      "recreating the Dataset/DataFrame involved.\n",
      "       \n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:661)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:330)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:389)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:227)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:35)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hasNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:955)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:383)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:138)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1516)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\t... 1 more\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/mnt/yarn/usercache/livy/appcache/application_1693814926276_0001/container_1693814926276_0001_01_000001/pyspark.zip/pyspark/sql/dataframe.py\", line 616, in show\n",
      "    print(self._jdf.showString(n, int_truncate, vertical))\n",
      "  File \"/mnt/yarn/usercache/livy/appcache/application_1693814926276_0001/container_1693814926276_0001_01_000001/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\", line 1322, in __call__\n",
      "    answer, self.gateway_client, self.target_id, self.name)\n",
      "  File \"/mnt/yarn/usercache/livy/appcache/application_1693814926276_0001/container_1693814926276_0001_01_000001/pyspark.zip/pyspark/sql/utils.py\", line 190, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/mnt/yarn/usercache/livy/appcache/application_1693814926276_0001/container_1693814926276_0001_01_000001/py4j-0.10.9.5-src.zip/py4j/protocol.py\", line 328, in get_return_value\n",
      "    format(target_id, \".\", name), value)\n",
      "py4j.protocol.Py4JJavaError: An error occurred while calling o566.showString.\n",
      ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 56.0 failed 4 times, most recent failure: Lost task 0.3 in stage 56.0 (TID 197) (ip-10-0-5-13.ec2.internal executor 9): java.io.FileNotFoundException: \n",
      "No such file or directory 's3://demobucketpavi/foodreceipe/interaction_level_df/part-00005-e5684401-5b85-4096-8c1e-388e2578132c-c000.snappy.parquet'\n",
      "\n",
      "It is possible the underlying files have been updated. You can explicitly invalidate\n",
      "the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by\n",
      "recreating the Dataset/DataFrame involved.\n",
      "       \n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:661)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:330)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:389)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:227)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:35)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hasNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:955)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:383)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:138)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1516)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2863)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2799)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2798)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2798)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1239)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1239)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1239)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3051)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2993)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1009)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2229)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2250)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2269)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:533)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:486)\n",
      "\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:48)\n",
      "\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3932)\n",
      "\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2904)\n",
      "\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:3922)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:554)\n",
      "\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3920)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:107)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:224)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.executeQuery$1(SQLExecution.scala:114)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$7(SQLExecution.scala:139)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:107)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:224)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:139)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:245)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:138)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:68)\n",
      "\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3920)\n",
      "\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2904)\n",
      "\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3125)\n",
      "\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:290)\n",
      "\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:329)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: java.io.FileNotFoundException: \n",
      "No such file or directory 's3://demobucketpavi/foodreceipe/interaction_level_df/part-00005-e5684401-5b85-4096-8c1e-388e2578132c-c000.snappy.parquet'\n",
      "\n",
      "It is possible the underlying files have been updated. You can explicitly invalidate\n",
      "the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by\n",
      "recreating the Dataset/DataFrame involved.\n",
      "       \n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:661)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:330)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:389)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:227)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:35)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hasNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:955)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:383)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:138)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1516)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\t... 1 more\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "interaction_level_df.filter('n_steps == 0').show(5, truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "22af25b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a68ea00903314d5786526c6c2971a322",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "An error occurred while calling o580.collectToPython.\n",
      ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 57.0 failed 4 times, most recent failure: Lost task 1.3 in stage 57.0 (TID 211) (ip-10-0-5-13.ec2.internal executor 9): org.apache.spark.sql.execution.datasources.FileDownloadException: Failed to download file path: s3://demobucketpavi/foodreceipe/interaction_level_df/part-00012-e5684401-5b85-4096-8c1e-388e2578132c-c000.snappy.parquet, range: 0-15370992, partition values: [empty row], isDataPresent: false, eTag: 41cdd51181a98dfc482a4f2611fc2e96-1\n",
      "\tat org.apache.spark.sql.execution.datasources.AsyncFileDownloader.next(AsyncFileDownloader.scala:142)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.getNextFile(FileScanRDD.scala:423)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:337)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:227)\n",
      "\tat org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:703)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithKeys_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:35)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hasNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:955)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.shuffle.sort.UnsafeShuffleWriter.write(UnsafeShuffleWriter.java:183)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:138)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1516)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "\tSuppressed: java.io.FileNotFoundException: No such file or directory 's3://demobucketpavi/foodreceipe/interaction_level_df/part-00001-e5684401-5b85-4096-8c1e-388e2578132c-c000.snappy.parquet'\n",
      "\t\tat com.amazon.ws.emr.hadoop.fs.s3n.S3NativeFileSystem.getFileStatus(S3NativeFileSystem.java:529)\n",
      "\t\tat com.amazon.ws.emr.hadoop.fs.EmrFileSystem.getFileStatus(EmrFileSystem.java:617)\n",
      "\t\tat org.apache.parquet.hadoop.util.HadoopInputFile.fromPath(HadoopInputFile.java:61)\n",
      "\t\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:39)\n",
      "\t\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat.$anonfun$buildPrefetcherWithPartitionValues$1(ParquetFileFormat.scala:537)\n",
      "\t\tat org.apache.spark.sql.execution.datasources.AsyncFileDownloader.org$apache$spark$sql$execution$datasources$AsyncFileDownloader$$downloadFile(AsyncFileDownloader.scala:93)\n",
      "\t\tat org.apache.spark.sql.execution.datasources.AsyncFileDownloader$$anon$1.call(AsyncFileDownloader.scala:73)\n",
      "\t\tat org.apache.spark.sql.execution.datasources.AsyncFileDownloader$$anon$1.call(AsyncFileDownloader.scala:72)\n",
      "\t\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n",
      "\t\t... 3 more\n",
      "Caused by: java.io.FileNotFoundException: No such file or directory 's3://demobucketpavi/foodreceipe/interaction_level_df/part-00012-e5684401-5b85-4096-8c1e-388e2578132c-c000.snappy.parquet'\n",
      "\tat com.amazon.ws.emr.hadoop.fs.s3n.S3NativeFileSystem.getFileStatus(S3NativeFileSystem.java:529)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.EmrFileSystem.getFileStatus(EmrFileSystem.java:617)\n",
      "\tat org.apache.parquet.hadoop.util.HadoopInputFile.fromPath(HadoopInputFile.java:61)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:39)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat.$anonfun$buildPrefetcherWithPartitionValues$1(ParquetFileFormat.scala:537)\n",
      "\tat org.apache.spark.sql.execution.datasources.AsyncFileDownloader.org$apache$spark$sql$execution$datasources$AsyncFileDownloader$$downloadFile(AsyncFileDownloader.scala:93)\n",
      "\tat org.apache.spark.sql.execution.datasources.AsyncFileDownloader$$anon$1.call(AsyncFileDownloader.scala:73)\n",
      "\tat org.apache.spark.sql.execution.datasources.AsyncFileDownloader$$anon$1.call(AsyncFileDownloader.scala:72)\n",
      "\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n",
      "\t... 3 more\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2863)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2799)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2798)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2798)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1239)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1239)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1239)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3051)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2993)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveExecutor.checkNoFailures(AdaptiveExecutor.scala:154)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveExecutor.doRun(AdaptiveExecutor.scala:88)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveExecutor.tryRunningAndGetFuture(AdaptiveExecutor.scala:66)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveExecutor.execute(AdaptiveExecutor.scala:57)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$getFinalPhysicalPlan$1(AdaptiveSparkPlanExec.scala:249)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.getFinalPhysicalPlan(AdaptiveSparkPlanExec.scala:248)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:521)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.executeCollect(AdaptiveSparkPlanExec.scala:483)\n",
      "\tat org.apache.spark.sql.Dataset.$anonfun$collectToPython$1(Dataset.scala:3756)\n",
      "\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:3922)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:554)\n",
      "\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3920)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:107)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:224)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.executeQuery$1(SQLExecution.scala:114)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$7(SQLExecution.scala:139)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:107)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:224)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:139)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:245)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:138)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:68)\n",
      "\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3920)\n",
      "\tat org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:3753)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.spark.sql.execution.datasources.FileDownloadException: Failed to download file path: s3://demobucketpavi/foodreceipe/interaction_level_df/part-00012-e5684401-5b85-4096-8c1e-388e2578132c-c000.snappy.parquet, range: 0-15370992, partition values: [empty row], isDataPresent: false, eTag: 41cdd51181a98dfc482a4f2611fc2e96-1\n",
      "\tat org.apache.spark.sql.execution.datasources.AsyncFileDownloader.next(AsyncFileDownloader.scala:142)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.getNextFile(FileScanRDD.scala:423)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:337)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:227)\n",
      "\tat org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:703)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithKeys_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:35)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hasNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:955)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.shuffle.sort.UnsafeShuffleWriter.write(UnsafeShuffleWriter.java:183)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:138)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1516)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\t... 1 more\n",
      "\tSuppressed: java.io.FileNotFoundException: No such file or directory 's3://demobucketpavi/foodreceipe/interaction_level_df/part-00001-e5684401-5b85-4096-8c1e-388e2578132c-c000.snappy.parquet'\n",
      "\t\tat com.amazon.ws.emr.hadoop.fs.s3n.S3NativeFileSystem.getFileStatus(S3NativeFileSystem.java:529)\n",
      "\t\tat com.amazon.ws.emr.hadoop.fs.EmrFileSystem.getFileStatus(EmrFileSystem.java:617)\n",
      "\t\tat org.apache.parquet.hadoop.util.HadoopInputFile.fromPath(HadoopInputFile.java:61)\n",
      "\t\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:39)\n",
      "\t\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat.$anonfun$buildPrefetcherWithPartitionValues$1(ParquetFileFormat.scala:537)\n",
      "\t\tat org.apache.spark.sql.execution.datasources.AsyncFileDownloader.org$apache$spark$sql$execution$datasources$AsyncFileDownloader$$downloadFile(AsyncFileDownloader.scala:93)\n",
      "\t\tat org.apache.spark.sql.execution.datasources.AsyncFileDownloader$$anon$1.call(AsyncFileDownloader.scala:73)\n",
      "\t\tat org.apache.spark.sql.execution.datasources.AsyncFileDownloader$$anon$1.call(AsyncFileDownloader.scala:72)\n",
      "\t\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n",
      "\t\t... 3 more\n",
      "Caused by: java.io.FileNotFoundException: No such file or directory 's3://demobucketpavi/foodreceipe/interaction_level_df/part-00012-e5684401-5b85-4096-8c1e-388e2578132c-c000.snappy.parquet'\n",
      "\tat com.amazon.ws.emr.hadoop.fs.s3n.S3NativeFileSystem.getFileStatus(S3NativeFileSystem.java:529)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.EmrFileSystem.getFileStatus(EmrFileSystem.java:617)\n",
      "\tat org.apache.parquet.hadoop.util.HadoopInputFile.fromPath(HadoopInputFile.java:61)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:39)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat.$anonfun$buildPrefetcherWithPartitionValues$1(ParquetFileFormat.scala:537)\n",
      "\tat org.apache.spark.sql.execution.datasources.AsyncFileDownloader.org$apache$spark$sql$execution$datasources$AsyncFileDownloader$$downloadFile(AsyncFileDownloader.scala:93)\n",
      "\tat org.apache.spark.sql.execution.datasources.AsyncFileDownloader$$anon$1.call(AsyncFileDownloader.scala:73)\n",
      "\tat org.apache.spark.sql.execution.datasources.AsyncFileDownloader$$anon$1.call(AsyncFileDownloader.scala:72)\n",
      "\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n",
      "\t... 3 more\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"<stdin>\", line 21, in get_n_items_satisfying_condition\n",
      "  File \"/mnt/yarn/usercache/livy/appcache/application_1693814926276_0001/container_1693814926276_0001_01_000001/pyspark.zip/pyspark/sql/dataframe.py\", line 1939, in first\n",
      "    return self.head()\n",
      "  File \"/mnt/yarn/usercache/livy/appcache/application_1693814926276_0001/container_1693814926276_0001_01_000001/pyspark.zip/pyspark/sql/dataframe.py\", line 1925, in head\n",
      "    rs = self.head(1)\n",
      "  File \"/mnt/yarn/usercache/livy/appcache/application_1693814926276_0001/container_1693814926276_0001_01_000001/pyspark.zip/pyspark/sql/dataframe.py\", line 1927, in head\n",
      "    return self.take(n)\n",
      "  File \"/mnt/yarn/usercache/livy/appcache/application_1693814926276_0001/container_1693814926276_0001_01_000001/pyspark.zip/pyspark/sql/dataframe.py\", line 869, in take\n",
      "    return self.limit(num).collect()\n",
      "  File \"/mnt/yarn/usercache/livy/appcache/application_1693814926276_0001/container_1693814926276_0001_01_000001/pyspark.zip/pyspark/sql/dataframe.py\", line 818, in collect\n",
      "    sock_info = self._jdf.collectToPython()\n",
      "  File \"/mnt/yarn/usercache/livy/appcache/application_1693814926276_0001/container_1693814926276_0001_01_000001/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\", line 1322, in __call__\n",
      "    answer, self.gateway_client, self.target_id, self.name)\n",
      "  File \"/mnt/yarn/usercache/livy/appcache/application_1693814926276_0001/container_1693814926276_0001_01_000001/pyspark.zip/pyspark/sql/utils.py\", line 190, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/mnt/yarn/usercache/livy/appcache/application_1693814926276_0001/container_1693814926276_0001_01_000001/py4j-0.10.9.5-src.zip/py4j/protocol.py\", line 328, in get_return_value\n",
      "    format(target_id, \".\", name), value)\n",
      "py4j.protocol.Py4JJavaError: An error occurred while calling o580.collectToPython.\n",
      ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 57.0 failed 4 times, most recent failure: Lost task 1.3 in stage 57.0 (TID 211) (ip-10-0-5-13.ec2.internal executor 9): org.apache.spark.sql.execution.datasources.FileDownloadException: Failed to download file path: s3://demobucketpavi/foodreceipe/interaction_level_df/part-00012-e5684401-5b85-4096-8c1e-388e2578132c-c000.snappy.parquet, range: 0-15370992, partition values: [empty row], isDataPresent: false, eTag: 41cdd51181a98dfc482a4f2611fc2e96-1\n",
      "\tat org.apache.spark.sql.execution.datasources.AsyncFileDownloader.next(AsyncFileDownloader.scala:142)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.getNextFile(FileScanRDD.scala:423)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:337)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:227)\n",
      "\tat org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:703)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithKeys_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:35)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hasNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:955)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.shuffle.sort.UnsafeShuffleWriter.write(UnsafeShuffleWriter.java:183)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:138)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1516)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "\tSuppressed: java.io.FileNotFoundException: No such file or directory 's3://demobucketpavi/foodreceipe/interaction_level_df/part-00001-e5684401-5b85-4096-8c1e-388e2578132c-c000.snappy.parquet'\n",
      "\t\tat com.amazon.ws.emr.hadoop.fs.s3n.S3NativeFileSystem.getFileStatus(S3NativeFileSystem.java:529)\n",
      "\t\tat com.amazon.ws.emr.hadoop.fs.EmrFileSystem.getFileStatus(EmrFileSystem.java:617)\n",
      "\t\tat org.apache.parquet.hadoop.util.HadoopInputFile.fromPath(HadoopInputFile.java:61)\n",
      "\t\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:39)\n",
      "\t\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat.$anonfun$buildPrefetcherWithPartitionValues$1(ParquetFileFormat.scala:537)\n",
      "\t\tat org.apache.spark.sql.execution.datasources.AsyncFileDownloader.org$apache$spark$sql$execution$datasources$AsyncFileDownloader$$downloadFile(AsyncFileDownloader.scala:93)\n",
      "\t\tat org.apache.spark.sql.execution.datasources.AsyncFileDownloader$$anon$1.call(AsyncFileDownloader.scala:73)\n",
      "\t\tat org.apache.spark.sql.execution.datasources.AsyncFileDownloader$$anon$1.call(AsyncFileDownloader.scala:72)\n",
      "\t\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n",
      "\t\t... 3 more\n",
      "Caused by: java.io.FileNotFoundException: No such file or directory 's3://demobucketpavi/foodreceipe/interaction_level_df/part-00012-e5684401-5b85-4096-8c1e-388e2578132c-c000.snappy.parquet'\n",
      "\tat com.amazon.ws.emr.hadoop.fs.s3n.S3NativeFileSystem.getFileStatus(S3NativeFileSystem.java:529)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.EmrFileSystem.getFileStatus(EmrFileSystem.java:617)\n",
      "\tat org.apache.parquet.hadoop.util.HadoopInputFile.fromPath(HadoopInputFile.java:61)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:39)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat.$anonfun$buildPrefetcherWithPartitionValues$1(ParquetFileFormat.scala:537)\n",
      "\tat org.apache.spark.sql.execution.datasources.AsyncFileDownloader.org$apache$spark$sql$execution$datasources$AsyncFileDownloader$$downloadFile(AsyncFileDownloader.scala:93)\n",
      "\tat org.apache.spark.sql.execution.datasources.AsyncFileDownloader$$anon$1.call(AsyncFileDownloader.scala:73)\n",
      "\tat org.apache.spark.sql.execution.datasources.AsyncFileDownloader$$anon$1.call(AsyncFileDownloader.scala:72)\n",
      "\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n",
      "\t... 3 more\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2863)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2799)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2798)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2798)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1239)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1239)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1239)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3051)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2993)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveExecutor.checkNoFailures(AdaptiveExecutor.scala:154)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveExecutor.doRun(AdaptiveExecutor.scala:88)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveExecutor.tryRunningAndGetFuture(AdaptiveExecutor.scala:66)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveExecutor.execute(AdaptiveExecutor.scala:57)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$getFinalPhysicalPlan$1(AdaptiveSparkPlanExec.scala:249)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.getFinalPhysicalPlan(AdaptiveSparkPlanExec.scala:248)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:521)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.executeCollect(AdaptiveSparkPlanExec.scala:483)\n",
      "\tat org.apache.spark.sql.Dataset.$anonfun$collectToPython$1(Dataset.scala:3756)\n",
      "\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:3922)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:554)\n",
      "\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3920)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:107)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:224)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.executeQuery$1(SQLExecution.scala:114)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$7(SQLExecution.scala:139)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:107)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:224)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:139)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:245)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:138)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:68)\n",
      "\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3920)\n",
      "\tat org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:3753)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.spark.sql.execution.datasources.FileDownloadException: Failed to download file path: s3://demobucketpavi/foodreceipe/interaction_level_df/part-00012-e5684401-5b85-4096-8c1e-388e2578132c-c000.snappy.parquet, range: 0-15370992, partition values: [empty row], isDataPresent: false, eTag: 41cdd51181a98dfc482a4f2611fc2e96-1\n",
      "\tat org.apache.spark.sql.execution.datasources.AsyncFileDownloader.next(AsyncFileDownloader.scala:142)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.getNextFile(FileScanRDD.scala:423)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:337)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:227)\n",
      "\tat org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:703)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithKeys_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:35)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hasNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:955)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.shuffle.sort.UnsafeShuffleWriter.write(UnsafeShuffleWriter.java:183)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:138)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1516)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\t... 1 more\n",
      "\tSuppressed: java.io.FileNotFoundException: No such file or directory 's3://demobucketpavi/foodreceipe/interaction_level_df/part-00001-e5684401-5b85-4096-8c1e-388e2578132c-c000.snappy.parquet'\n",
      "\t\tat com.amazon.ws.emr.hadoop.fs.s3n.S3NativeFileSystem.getFileStatus(S3NativeFileSystem.java:529)\n",
      "\t\tat com.amazon.ws.emr.hadoop.fs.EmrFileSystem.getFileStatus(EmrFileSystem.java:617)\n",
      "\t\tat org.apache.parquet.hadoop.util.HadoopInputFile.fromPath(HadoopInputFile.java:61)\n",
      "\t\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:39)\n",
      "\t\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat.$anonfun$buildPrefetcherWithPartitionValues$1(ParquetFileFormat.scala:537)\n",
      "\t\tat org.apache.spark.sql.execution.datasources.AsyncFileDownloader.org$apache$spark$sql$execution$datasources$AsyncFileDownloader$$downloadFile(AsyncFileDownloader.scala:93)\n",
      "\t\tat org.apache.spark.sql.execution.datasources.AsyncFileDownloader$$anon$1.call(AsyncFileDownloader.scala:73)\n",
      "\t\tat org.apache.spark.sql.execution.datasources.AsyncFileDownloader$$anon$1.call(AsyncFileDownloader.scala:72)\n",
      "\t\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n",
      "\t\t... 3 more\n",
      "Caused by: java.io.FileNotFoundException: No such file or directory 's3://demobucketpavi/foodreceipe/interaction_level_df/part-00012-e5684401-5b85-4096-8c1e-388e2578132c-c000.snappy.parquet'\n",
      "\tat com.amazon.ws.emr.hadoop.fs.s3n.S3NativeFileSystem.getFileStatus(S3NativeFileSystem.java:529)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.EmrFileSystem.getFileStatus(EmrFileSystem.java:617)\n",
      "\tat org.apache.parquet.hadoop.util.HadoopInputFile.fromPath(HadoopInputFile.java:61)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:39)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat.$anonfun$buildPrefetcherWithPartitionValues$1(ParquetFileFormat.scala:537)\n",
      "\tat org.apache.spark.sql.execution.datasources.AsyncFileDownloader.org$apache$spark$sql$execution$datasources$AsyncFileDownloader$$downloadFile(AsyncFileDownloader.scala:93)\n",
      "\tat org.apache.spark.sql.execution.datasources.AsyncFileDownloader$$anon$1.call(AsyncFileDownloader.scala:73)\n",
      "\tat org.apache.spark.sql.execution.datasources.AsyncFileDownloader$$anon$1.call(AsyncFileDownloader.scala:72)\n",
      "\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n",
      "\t... 3 more\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "get_n_items_satisfying_condition(df = interaction_level_df,\n",
    "                                 condition = 'n_steps == 0',\n",
    "                                 aggregation_level = \"recipe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "de44d004",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "658f92f3da3b48e597f4a023ed60ac92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Remove recipes with n_steps zero\n",
    "\n",
    "interaction_level_df = interaction_level_df.filter(\"n_steps > 0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "df340d09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "901ddb1903f948b4a7a2df418455db15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "Pandas >= 1.0.5 must be installed; however, your version was 0.25.1.\n",
      "Traceback (most recent call last):\n",
      "  File \"<stdin>\", line 43, in bucket_col_print_summary\n",
      "  File \"/mnt/yarn/usercache/livy/appcache/application_1693814926276_0001/container_1693814926276_0001_01_000001/pyspark.zip/pyspark/sql/pandas/conversion.py\", line 86, in toPandas\n",
      "    require_minimum_pandas_version()\n",
      "  File \"/mnt/yarn/usercache/livy/appcache/application_1693814926276_0001/container_1693814926276_0001_01_000001/pyspark.zip/pyspark/sql/pandas/utils.py\", line 40, in require_minimum_pandas_version\n",
      "    \"your version was %s.\" % (minimum_pandas_version, pandas.__version__)\n",
      "ImportError: Pandas >= 1.0.5 must be installed; however, your version was 0.25.1.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "splits = [0, 2, 6, 8, 12, 29, float('Inf')]\n",
    "inputCol  = \"n_steps\"\n",
    "outputCol = \"n_steps_bucket\"\n",
    "\n",
    "(interaction_level_df, n_steps_bucketizer, n_steps_pandas_df) = bucket_col_print_summary(df = interaction_level_df,\n",
    "                                                                              splits = splits,\n",
    "                                                                              inputCol  = inputCol,\n",
    "                                                                              outputCol = outputCol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "14ded29c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplot plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6332e3c3",
   "metadata": {},
   "source": [
    "**4. `n_ingredients`**\n",
    "- Not relevant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "ce8e1030",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad036bf77aa14a20bf15b3598f6f3367",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0.0: 1, 0.01: 2.0, 0.25: 6.0, 0.5: 9.0, 0.75: 11.0, 0.99: 20.0, 1.0: 43}"
     ]
    }
   ],
   "source": [
    "get_quantiles(df = interaction_level_df,\n",
    "              col_name = \"n_ingredients\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "d7ed08af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "048df1005a594613aca6eb7736ce6801",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added bucketized column n_ingredients_bucket\n",
      "\n",
      "Bucketing done for split definition: [0, 6, 9, 11, inf]\n",
      "\n",
      "Printing summary statistics for ratings in buckets below:"
     ]
    }
   ],
   "source": [
    "splits = [0, 6, 9, 11, float('Inf')]\n",
    "inputCol  = \"n_ingredients\"\n",
    "outputCol = \"n_ingredients_bucket\"\n",
    "\n",
    "(interaction_level_df, n_ingredients_bucketizer, n_ingredients_pandas_df) = bucket_col_print_summary(df = interaction_level_df,\n",
    "                                                                              splits = splits,\n",
    "                                                                              inputCol  = inputCol,\n",
    "                                                                              outputCol = outputCol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "a5dc708c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplot plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4dadbbf",
   "metadata": {},
   "source": [
    "**5. `nutrition` columns**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ec0b11",
   "metadata": {},
   "source": [
    "- `calories` - Calories per serving seems irrelevant\n",
    "- `fat (per 100 cal)` - Calories per serving seems irrelevant\n",
    "- `sugar (per 100 cal)` - Calories per serving seems irrelevant\n",
    "- `sodium (per 100 cal)` - Calories per serving seems irrelevant\n",
    "- `protein (per 100 cal)` - Calories per serving seems irrelevant\n",
    "- `sat. fat (per 100 cal)` - Calories per serving seems irrelevant\n",
    "- `carbs (per 100 cal)` - Calories per serving seems irrelevant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "177938af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b114c52017ea4bdf8e476cc41508b67c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['user_id', 'recipe_id', 'rating', 'review', 'review_date', 'name', 'id', 'minutes', 'contributor_id', 'submitted', 'tags', 'nutrition', 'n_steps', 'steps', 'description', 'ingredients', 'n_ingredients', 'calories', 'total_fat_PDV', 'sugar_PDV', 'sodium_PDV', 'protein_PDV', 'saturated_fat_PDV', 'carbohydrates_PDV', 'calories_per_100_cal', 'total_fat_per_100_cal', 'sugar_per_100_cal', 'sodium_per_100_cal', 'protein_per_100_cal', 'saturated_fat_per_100_cal', 'carbohydrates_per_100_cal', 'days_since_submission_on_review_date', 'months_since_submission_on_review_date', 'years_since_submission_on_review_date', 'years_since_submission_on_review_date_bucket', 'prep_time_bucket', 'n_steps_bucket', 'n_ingredients_bucket']"
     ]
    }
   ],
   "source": [
    "interaction_level_df.columns \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "f5403a11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44f83a1bb4bf4a3ba1673526255ab45b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nutrition_cols = ['calories', \n",
    "                  'total_fat_PDV', \n",
    "                  'sugar_PDV', \n",
    "                  'sodium_PDV', \n",
    "                  'protein_PDV', \n",
    "                  'saturated_fat_PDV', \n",
    "                  'carbohydrates_PDV', \n",
    "                  'total_fat_per_100_cal', \n",
    "                  'sugar_per_100_cal', \n",
    "                  'sodium_per_100_cal', \n",
    "                  'protein_per_100_cal', \n",
    "                  'saturated_fat_per_100_cal', \n",
    "                  'carbohydrates_per_100_cal']\n",
    "\n",
    "quantiles_list = [0.00, 0.05, 0.25, 0.5, 0.75, 0.95, 1.00]\n",
    "nutrition_col_quantiles = pd.DataFrame(index = quantiles_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "ea51b34e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46a08f75ccb34526867af80c753298ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for col in nutrition_cols:\n",
    "    nutrition_col_quantiles[col] = (get_quantiles(df = interaction_level_df,\n",
    "                                                col_name = col,\n",
    "                                                quantiles_list=quantiles_list)\n",
    "                                  .values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "b3ee770e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3e5b1a9757740c78301133ca4855a1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nutrition_col_quantile_summary = pd.DataFrame(index = [\"0.00-0.25\", \"0.25-0.50\", \"0.50-0.75\", \"0.75-0.95\", \"0.95 - 1.00\"])\n",
    "\n",
    "for col in nutrition_cols:\n",
    "    splits = ([0]\n",
    "            + list(nutrition_col_quantiles.loc[[0.25, 0.5, 0.75, 0.95], col].round())\n",
    "            + [float('Inf')])\n",
    "    inputCol  = col\n",
    "    outputCol = col+\"_bucket\"\n",
    "\n",
    "    if outputCol in interaction_level_df.columns:\n",
    "        interaction_level_df = interaction_level_df.drop(outputCol)\n",
    "\n",
    "  # Training bucketizer\n",
    "    bucketizer = Bucketizer(splits = splits,\n",
    "                          inputCol  = inputCol,\n",
    "                          outputCol = outputCol)\n",
    "  \n",
    "    interaction_level_df = bucketizer.setHandleInvalid(\"keep\").transform(interaction_level_df)\n",
    "  \n",
    "    nutrition_col_quantile_summary.loc[:, col] = (interaction_level_df\n",
    "                                                .groupBy(outputCol)\n",
    "                                                .agg(F.avg('rating').alias('avg_rating'))\n",
    "                                                .sort(outputCol)\n",
    "                                                .select('avg_rating').toPandas().values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "10ea621d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ced6c98383545719e40f375c7006805",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# set the max columns to none\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "19d2bd55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8db44629a1d4b06b6065f4e2c88bd3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             calories  total_fat_PDV  sugar_PDV  sodium_PDV  protein_PDV  \\\n",
      "0.00-0.25    4.416167       4.393560   4.416368    4.423843     4.422679   \n",
      "0.25-0.50    4.428239       4.420946   4.434610    4.408631     4.419234   \n",
      "0.50-0.75    4.418471       4.427758   4.403005    4.422298     4.410517   \n",
      "0.75-0.95    4.393681       4.411867   4.406985    4.397199     4.399930   \n",
      "0.95 - 1.00  4.342026       4.371152   4.332979    4.373164     4.372771   \n",
      "\n",
      "             saturated_fat_PDV  carbohydrates_PDV  total_fat_per_100_cal  \\\n",
      "0.00-0.25             4.396849           4.439453               4.385130   \n",
      "0.25-0.50             4.422079           4.421594               4.396544   \n",
      "0.50-0.75             4.423550           4.417625               4.415027   \n",
      "0.75-0.95             4.414637           4.382006               4.438629   \n",
      "0.95 - 1.00           4.351969           4.324922               4.476848   \n",
      "\n",
      "             sugar_per_100_cal  sodium_per_100_cal  protein_per_100_cal  \\\n",
      "0.00-0.25             4.412924            4.417770             4.412914   \n",
      "0.25-0.50             4.427622            4.398280             4.413150   \n",
      "0.50-0.75             4.410983            4.423546             4.416714   \n",
      "0.75-0.95             4.392021            4.414099             4.402535   \n",
      "0.95 - 1.00           4.413383            4.389711             4.404998   \n",
      "\n",
      "             saturated_fat_per_100_cal  carbohydrates_per_100_cal  \n",
      "0.00-0.25                     4.394655                   4.438210  \n",
      "0.25-0.50                     4.405957                   4.419843  \n",
      "0.50-0.75                     4.412177                   4.400808  \n",
      "0.75-0.95                     4.429877                   4.379254  \n",
      "0.95 - 1.00                   4.446134                   4.399911"
     ]
    }
   ],
   "source": [
    "nutrition_col_quantile_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8eab7800",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cecce6507cc4eec887f4cd6f133eaaa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "An error occurred while calling o634.parquet.\n",
      ": org.apache.spark.SparkException: Job aborted.\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.jobAbortedError(QueryExecutionErrors.scala:651)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:279)\n",
      "\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:193)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:104)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:107)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:224)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.executeQuery$1(SQLExecution.scala:114)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$7(SQLExecution.scala:139)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:107)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:224)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:139)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:245)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:138)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:68)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:101)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:97)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:626)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:179)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:626)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:31)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:602)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:97)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:84)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:82)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:125)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:860)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:390)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:363)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:793)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 58.0 failed 4 times, most recent failure: Lost task 1.3 in stage 58.0 (TID 226) (ip-10-0-5-13.ec2.internal executor 10): java.io.FileNotFoundException: \n",
      "No such file or directory 's3://demobucketpavi/foodreceipe/interaction_level_df/part-00014-e5684401-5b85-4096-8c1e-388e2578132c-c000.snappy.parquet'\n",
      "\n",
      "It is possible the underlying files have been updated. You can explicitly invalidate\n",
      "the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by\n",
      "recreating the Dataset/DataFrame involved.\n",
      "       \n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:661)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:330)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:389)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:227)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:35)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hasNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:955)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:314)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$21(FileFormatWriter.scala:257)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:138)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1516)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2863)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2799)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2798)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2798)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1239)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1239)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1239)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3051)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2993)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1009)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2229)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:246)\n",
      "\t... 47 more\n",
      "Caused by: java.io.FileNotFoundException: \n",
      "No such file or directory 's3://demobucketpavi/foodreceipe/interaction_level_df/part-00014-e5684401-5b85-4096-8c1e-388e2578132c-c000.snappy.parquet'\n",
      "\n",
      "It is possible the underlying files have been updated. You can explicitly invalidate\n",
      "the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by\n",
      "recreating the Dataset/DataFrame involved.\n",
      "       \n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:661)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:330)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:389)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:227)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:35)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hasNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:955)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:314)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$21(FileFormatWriter.scala:257)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:138)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1516)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\t... 1 more\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/mnt/yarn/usercache/livy/appcache/application_1693814926276_0001/container_1693814926276_0001_01_000001/pyspark.zip/pyspark/sql/readwriter.py\", line 1140, in parquet\n",
      "    self._jwrite.parquet(path)\n",
      "  File \"/mnt/yarn/usercache/livy/appcache/application_1693814926276_0001/container_1693814926276_0001_01_000001/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\", line 1322, in __call__\n",
      "    answer, self.gateway_client, self.target_id, self.name)\n",
      "  File \"/mnt/yarn/usercache/livy/appcache/application_1693814926276_0001/container_1693814926276_0001_01_000001/pyspark.zip/pyspark/sql/utils.py\", line 190, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/mnt/yarn/usercache/livy/appcache/application_1693814926276_0001/container_1693814926276_0001_01_000001/py4j-0.10.9.5-src.zip/py4j/protocol.py\", line 328, in get_return_value\n",
      "    format(target_id, \".\", name), value)\n",
      "py4j.protocol.Py4JJavaError: An error occurred while calling o634.parquet.\n",
      ": org.apache.spark.SparkException: Job aborted.\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.jobAbortedError(QueryExecutionErrors.scala:651)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:279)\n",
      "\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:193)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:104)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:107)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:224)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.executeQuery$1(SQLExecution.scala:114)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$7(SQLExecution.scala:139)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:107)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:224)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:139)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:245)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:138)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:68)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:101)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:97)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:626)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:179)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:626)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:31)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:602)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:97)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:84)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:82)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:125)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:860)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:390)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:363)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:793)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 58.0 failed 4 times, most recent failure: Lost task 1.3 in stage 58.0 (TID 226) (ip-10-0-5-13.ec2.internal executor 10): java.io.FileNotFoundException: \n",
      "No such file or directory 's3://demobucketpavi/foodreceipe/interaction_level_df/part-00014-e5684401-5b85-4096-8c1e-388e2578132c-c000.snappy.parquet'\n",
      "\n",
      "It is possible the underlying files have been updated. You can explicitly invalidate\n",
      "the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by\n",
      "recreating the Dataset/DataFrame involved.\n",
      "       \n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:661)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:330)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:389)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:227)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:35)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hasNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:955)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:314)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$21(FileFormatWriter.scala:257)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:138)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1516)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2863)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2799)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2798)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2798)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1239)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1239)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1239)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3051)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2993)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1009)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2229)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:246)\n",
      "\t... 47 more\n",
      "Caused by: java.io.FileNotFoundException: \n",
      "No such file or directory 's3://demobucketpavi/foodreceipe/interaction_level_df/part-00014-e5684401-5b85-4096-8c1e-388e2578132c-c000.snappy.parquet'\n",
      "\n",
      "It is possible the underlying files have been updated. You can explicitly invalidate\n",
      "the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by\n",
      "recreating the Dataset/DataFrame involved.\n",
      "       \n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:661)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:330)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:389)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:227)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:35)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hasNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:955)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:314)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$21(FileFormatWriter.scala:257)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:138)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1516)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\t... 1 more\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Writing the modified data to S3 \n",
    "\n",
    "interaction_level_df.write.mode('overwrite').parquet('s3://demobucketpavi/foodreceipe/interaction_level_df_postEDA.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "413017c3",
   "metadata": {},
   "source": [
    "#### <center> End of notebook 2 </center>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
