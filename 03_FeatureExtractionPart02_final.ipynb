{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e71f2dc",
   "metadata": {},
   "source": [
    "## Initial Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a40ad05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The code failed because of a fatal error:\n",
      "\tSession 2 unexpectedly reached final status 'dead'. See logs:\n",
      "stdout: \n",
      "\n",
      "stderr: \n",
      "23/09/04 10:43:24 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "23/09/04 10:43:24 INFO DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at ip-10-0-5-13.ec2.internal/10.0.5.13:8032\n",
      "23/09/04 10:43:25 INFO Configuration: resource-types.xml not found\n",
      "23/09/04 10:43:25 INFO ResourceUtils: Unable to find 'resource-types.xml'.\n",
      "23/09/04 10:43:25 INFO Client: Verifying our application has not requested more than the maximum memory capability of the cluster (12288 MB per container)\n",
      "23/09/04 10:43:25 INFO Client: Will allocate AM container, with 2432 MB memory including 384 MB overhead\n",
      "23/09/04 10:43:25 INFO Client: Setting up container launch context for our AM\n",
      "23/09/04 10:43:25 INFO Client: Setting up the launch environment for our AM container\n",
      "23/09/04 10:43:25 INFO Client: Preparing resources for our AM container\n",
      "23/09/04 10:43:25 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n",
      "23/09/04 10:43:29 INFO Client: Uploading resource file:/mnt/tmp/spark-5675a8e7-584b-4872-b164-d2046df0da55/__spark_libs__3837436133610880387.zip -> hdfs://ip-10-0-5-13.ec2.internal:8020/user/livy/.sparkStaging/application_1693814926276_0003/__spark_libs__3837436133610880387.zip\n",
      "23/09/04 10:43:33 INFO Client: Uploading resource file:/usr/lib/livy/rsc-jars/arpack_combined_all-0.1.jar -> hdfs://ip-10-0-5-13.ec2.internal:8020/user/livy/.sparkStaging/application_1693814926276_0003/arpack_combined_all-0.1.jar\n",
      "23/09/04 10:43:33 INFO Client: Uploading resource file:/usr/lib/livy/rsc-jars/core-1.1.2.jar -> hdfs://ip-10-0-5-13.ec2.internal:8020/user/livy/.sparkStaging/application_1693814926276_0003/core-1.1.2.jar\n",
      "23/09/04 10:43:33 INFO Client: Uploading resource file:/usr/lib/livy/rsc-jars/jniloader-1.1.jar -> hdfs://ip-10-0-5-13.ec2.internal:8020/user/livy/.sparkStaging/application_1693814926276_0003/jniloader-1.1.jar\n",
      "23/09/04 10:43:33 INFO Client: Uploading resource file:/usr/lib/livy/rsc-jars/livy-api-0.7.1-incubating.jar -> hdfs://ip-10-0-5-13.ec2.internal:8020/user/livy/.sparkStaging/application_1693814926276_0003/livy-api-0.7.1-incubating.jar\n",
      "23/09/04 10:43:33 INFO Client: Uploading resource file:/usr/lib/livy/rsc-jars/livy-rsc-0.7.1-incubating.jar -> hdfs://ip-10-0-5-13.ec2.internal:8020/user/livy/.sparkStaging/application_1693814926276_0003/livy-rsc-0.7.1-incubating.jar\n",
      "23/09/04 10:43:33 INFO Client: Uploading resource file:/usr/lib/livy/rsc-jars/livy-thriftserver-session-0.7.1-incubating.jar -> hdfs://ip-10-0-5-13.ec2.internal:8020/user/livy/.sparkStaging/application_1693814926276_0003/livy-thriftserver-session-0.7.1-incubating.jar\n",
      "23/09/04 10:43:34 INFO Client: Uploading resource file:/usr/lib/livy/rsc-jars/native_ref-java-1.1.jar -> hdfs://ip-10-0-5-13.ec2.internal:8020/user/livy/.sparkStaging/application_1693814926276_0003/native_ref-java-1.1.jar\n",
      "23/09/04 10:43:34 INFO Client: Uploading resource file:/usr/lib/livy/rsc-jars/native_system-java-1.1.jar -> hdfs://ip-10-0-5-13.ec2.internal:8020/user/livy/.sparkStaging/application_1693814926276_0003/native_system-java-1.1.jar\n",
      "23/09/04 10:43:34 INFO Client: Uploading resource file:/usr/lib/livy/rsc-jars/netlib-native_ref-linux-armhf-1.1-natives.jar -> hdfs://ip-10-0-5-13.ec2.internal:8020/user/livy/.sparkStaging/application_1693814926276_0003/netlib-native_ref-linux-armhf-1.1-natives.jar\n",
      "23/09/04 10:43:34 INFO Client: Uploading resource file:/usr/lib/livy/rsc-jars/netlib-native_ref-linux-i686-1.1-natives.jar -> hdfs://ip-10-0-5-13.ec2.internal:8020/user/livy/.sparkStaging/application_1693814926276_0003/netlib-native_ref-linux-i686-1.1-natives.jar\n",
      "23/09/04 10:43:34 INFO Client: Uploading resource file:/usr/lib/livy/rsc-jars/netlib-native_ref-linux-x86_64-1.1-natives.jar -> hdfs://ip-10-0-5-13.ec2.internal:8020/user/livy/.sparkStaging/application_1693814926276_0003/netlib-native_ref-linux-x86_64-1.1-natives.jar\n",
      "23/09/04 10:43:34 INFO Client: Uploading resource file:/usr/lib/livy/rsc-jars/netlib-native_ref-osx-x86_64-1.1-natives.jar -> hdfs://ip-10-0-5-13.ec2.internal:8020/user/livy/.sparkStaging/application_1693814926276_0003/netlib-native_ref-osx-x86_64-1.1-natives.jar\n",
      "23/09/04 10:43:34 INFO Client: Uploading resource file:/usr/lib/livy/rsc-jars/netlib-native_ref-win-i686-1.1-natives.jar -> hdfs://ip-10-0-5-13.ec2.internal:8020/user/livy/.sparkStaging/application_1693814926276_0003/netlib-native_ref-win-i686-1.1-natives.jar\n",
      "23/09/04 10:43:34 INFO Client: Uploading resource file:/usr/lib/livy/rsc-jars/netlib-native_ref-win-x86_64-1.1-natives.jar -> hdfs://ip-10-0-5-13.ec2.internal:8020/user/livy/.sparkStaging/application_1693814926276_0003/netlib-native_ref-win-x86_64-1.1-natives.jar\n",
      "23/09/04 10:43:34 INFO Client: Uploading resource file:/usr/lib/livy/rsc-jars/netlib-native_system-linux-armhf-1.1-natives.jar -> hdfs://ip-10-0-5-13.ec2.internal:8020/user/livy/.sparkStaging/application_1693814926276_0003/netlib-native_system-linux-armhf-1.1-natives.jar\n",
      "23/09/04 10:43:34 INFO Client: Uploading resource file:/usr/lib/livy/rsc-jars/netlib-native_system-linux-i686-1.1-natives.jar -> hdfs://ip-10-0-5-13.ec2.internal:8020/user/livy/.sparkStaging/application_1693814926276_0003/netlib-native_system-linux-i686-1.1-natives.jar\n",
      "23/09/04 10:43:34 INFO Client: Uploading resource file:/usr/lib/livy/rsc-jars/netlib-native_system-linux-x86_64-1.1-natives.jar -> hdfs://ip-10-0-5-13.ec2.internal:8020/user/livy/.sparkStaging/application_1693814926276_0003/netlib-native_system-linux-x86_64-1.1-natives.jar\n",
      "23/09/04 10:43:34 INFO Client: Uploading resource file:/usr/lib/livy/rsc-jars/netlib-native_system-osx-x86_64-1.1-natives.jar -> hdfs://ip-10-0-5-13.ec2.internal:8020/user/livy/.sparkStaging/application_1693814926276_0003/netlib-native_system-osx-x86_64-1.1-natives.jar\n",
      "23/09/04 10:43:34 INFO Client: Uploading resource file:/usr/lib/livy/rsc-jars/netlib-native_system-win-i686-1.1-natives.jar -> hdfs://ip-10-0-5-13.ec2.internal:8020/user/livy/.sparkStaging/application_1693814926276_0003/netlib-native_system-win-i686-1.1-natives.jar\n",
      "23/09/04 10:43:34 INFO Client: Uploading resource file:/usr/lib/livy/rsc-jars/netlib-native_system-win-x86_64-1.1-natives.jar -> hdfs://ip-10-0-5-13.ec2.internal:8020/user/livy/.sparkStaging/application_1693814926276_0003/netlib-native_system-win-x86_64-1.1-natives.jar\n",
      "23/09/04 10:43:34 INFO Client: Uploading resource file:/usr/lib/livy/rsc-jars/netty-all-4.1.17.Final.jar -> hdfs://ip-10-0-5-13.ec2.internal:8020/user/livy/.sparkStaging/application_1693814926276_0003/netty-all-4.1.17.Final.jar\n",
      "23/09/04 10:43:34 INFO Client: Uploading resource file:/usr/lib/livy/repl_2.12-jars/commons-codec-1.9.jar -> hdfs://ip-10-0-5-13.ec2.internal:8020/user/livy/.sparkStaging/application_1693814926276_0003/commons-codec-1.9.jar\n",
      "23/09/04 10:43:34 INFO Client: Uploading resource file:/usr/lib/livy/repl_2.12-jars/livy-core_2.12-0.7.1-incubating.jar -> hdfs://ip-10-0-5-13.ec2.internal:8020/user/livy/.sparkStaging/application_1693814926276_0003/livy-core_2.12-0.7.1-incubating.jar\n",
      "23/09/04 10:43:34 INFO Client: Uploading resource file:/usr/lib/livy/repl_2.12-jars/livy-repl_2.12-0.7.1-incubating.jar -> hdfs://ip-10-0-5-13.ec2.internal:8020/user/livy/.sparkStaging/application_1693814926276_0003/livy-repl_2.12-0.7.1-incubating.jar\n",
      "23/09/04 10:43:34 INFO Client: Uploading resource file:/etc/spark/conf.dist/hive-site.xml -> hdfs://ip-10-0-5-13.ec2.internal:8020/user/livy/.sparkStaging/application_1693814926276_0003/hive-site.xml\n",
      "23/09/04 10:43:34 INFO Client: Uploading resource file:/etc/hudi/conf.dist/hudi-defaults.conf -> hdfs://ip-10-0-5-13.ec2.internal:8020/user/livy/.sparkStaging/application_1693814926276_0003/hudi-defaults.conf\n",
      "23/09/04 10:43:34 INFO Client: Uploading resource file:/usr/lib/spark/R/lib/sparkr.zip#sparkr -> hdfs://ip-10-0-5-13.ec2.internal:8020/user/livy/.sparkStaging/application_1693814926276_0003/sparkr.zip\n",
      "23/09/04 10:43:35 INFO Client: Uploading resource file:/usr/lib/spark/python/lib/pyspark.zip -> hdfs://ip-10-0-5-13.ec2.internal:8020/user/livy/.sparkStaging/application_1693814926276_0003/pyspark.zip\n",
      "23/09/04 10:43:35 INFO Client: Uploading resource file:/usr/lib/spark/python/lib/py4j-0.10.9.5-src.zip -> hdfs://ip-10-0-5-13.ec2.internal:8020/user/livy/.sparkStaging/application_1693814926276_0003/py4j-0.10.9.5-src.zip\n",
      "23/09/04 10:43:35 INFO Client: Uploading resource file:/mnt/tmp/spark-5675a8e7-584b-4872-b164-d2046df0da55/__spark_conf__4664836315644198653.zip -> hdfs://ip-10-0-5-13.ec2.internal:8020/user/livy/.sparkStaging/application_1693814926276_0003/__spark_conf__.zip\n",
      "23/09/04 10:43:35 INFO SecurityManager: Changing view acls to: livy\n",
      "23/09/04 10:43:35 INFO SecurityManager: Changing modify acls to: livy\n",
      "23/09/04 10:43:35 INFO SecurityManager: Changing view acls groups to: \n",
      "23/09/04 10:43:35 INFO SecurityManager: Changing modify acls groups to: \n",
      "23/09/04 10:43:35 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(livy); groups with view permissions: Set(); users  with modify permissions: Set(livy); groups with modify permissions: Set()\n",
      "23/09/04 10:43:35 INFO Client: Submitting application application_1693814926276_0003 to ResourceManager\n",
      "23/09/04 10:43:35 INFO YarnClientImpl: Submitted application application_1693814926276_0003\n",
      "23/09/04 10:43:35 INFO Client: Application report for application_1693814926276_0003 (state: ACCEPTED)\n",
      "23/09/04 10:43:35 INFO Client: \n",
      "\t client token: N/A\n",
      "\t diagnostics: [Mon Sep 04 10:43:35 +0000 2023] Application is added to the scheduler and is not yet activated. Queue's AM resource limit exceeded.  Details : AM Partition = <DEFAULT_PARTITION>; AM Resource Request = <memory:2432, max memory:12288, vCores:1, max vCores:8>; Queue Resource Limit for AM = <memory:6144, vCores:1>; User AM Resource Limit of the queue = <memory:6144, vCores:1>; Queue AM Resource Usage = <memory:4864, vCores:2>; \n",
      "\t ApplicationMaster host: N/A\n",
      "\t ApplicationMaster RPC port: -1\n",
      "\t queue: default\n",
      "\t start time: 1693824215522\n",
      "\t final status: UNDEFINED\n",
      "\t tracking URL: http://ip-10-0-5-13.ec2.internal:20888/proxy/application_1693814926276_0003/\n",
      "\t user: livy\n",
      "23/09/04 10:43:35 INFO ShutdownHookManager: Shutdown hook called\n",
      "23/09/04 10:43:35 INFO ShutdownHookManager: Deleting directory /mnt/tmp/spark-c1e81ff6-fd82-4d00-b95c-dbd1ef0517dc\n",
      "23/09/04 10:43:35 INFO ShutdownHookManager: Deleting directory /mnt/tmp/spark-5675a8e7-584b-4872-b164-d2046df0da55\n",
      "\n",
      "YARN Diagnostics: \n",
      "Application application_1693814926276_0003 was killed by user livy at 10.0.5.13.\n",
      "\n",
      "Some things to try:\n",
      "a) Make sure Spark has enough available resources for Jupyter to create a Spark context.\n",
      "b) Contact your Jupyter administrator to make sure the Spark magics library is configured correctly.\n",
      "c) Restart the kernel.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e6d1f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"Basics\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b16e5a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccbec7ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this everytime you create a new spark instance. \n",
    "\n",
    "spark.sparkContext.install_pypi_package(\"plotly==5.5.0\")\n",
    "spark.sparkContext.install_pypi_package(\"pandas==0.25.1\")\n",
    "spark.sparkContext.install_pypi_package(\"numpy==1.14.5\")\n",
    "spark.sparkContext.install_pypi_package(\"matplotlib==3.1.1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea1e671",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.ml.feature import Bucketizer\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Import for typecasting columns\n",
    "from pyspark.sql.types import IntegerType,BooleanType,DateType,FloatType,StringType\n",
    "from pyspark.sql.types import ArrayType"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b75577",
   "metadata": {},
   "source": [
    "## Defining Custom Functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "515ff4f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_quantiles(df, col_name, quantiles_list = [0.01, 0.25, 0.5, 0.75, 0.99]):\n",
    "    \"\"\"\n",
    "    Takes a numerical column and returns column values at requested quantiles\n",
    "\n",
    "    Inputs \n",
    "    Argument 1: Dataframe\n",
    "    Argument 2: Name of the column\n",
    "    Argument 3: A list of quantiles you want to find. Default value [0.01, 0.25, 0.5, 0.75, 0.99]\n",
    "\n",
    "    Output \n",
    "    Returns a dictionary with quantiles as keys and column quantile values as values \n",
    "    \"\"\"\n",
    "    # Get min, max and quantile values for given column\n",
    "    min_val = df.agg(F.min(col_name)).first()[0]\n",
    "    max_val = df.agg(F.max(col_name)).first()[0]\n",
    "    quantiles_vals = df.approxQuantile(col_name,\n",
    "                                       quantiles_list,\n",
    "                                       0)\n",
    "  \n",
    "    # Store min, quantiles and max in output dict, sequentially\n",
    "    quantiles_dict = {0.0:min_val}\n",
    "    quantiles_dict.update(dict(zip(quantiles_list, quantiles_vals)))\n",
    "    quantiles_dict.update({1.0:max_val})\n",
    "    return(quantiles_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca215059",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_bucketwise_statistics (summary, bucketizer):\n",
    "    \"\"\"\n",
    "    Takes in a dataframe and a bucketizer object and plots the summary statistics for each bucket in the dataframe. \n",
    "  \n",
    "    Inputs\n",
    "    Argument 1: Pandas dataframe obtained from bucket_col_print_summary function \n",
    "    Argument 2: Bucketizer object obtained from bucket_col_print_summary function\n",
    "  \n",
    "    Output\n",
    "    Displays a plot of bucketwise average ratings nunber of ratings of a parameter.   \n",
    "    \"\"\"\n",
    "    # Creating bucket labels from splits\n",
    "    classlist = bucketizer.getSplits()\n",
    "    number_of_classes = len(classlist) - 1\n",
    "\n",
    "    class_labels = []\n",
    "    hover_labels = []\n",
    "    for i in range (number_of_classes):\n",
    "        hover_labels.append(str(classlist[i])+\"-\"+str(classlist[i+1]) +\" (Bucket name: \"+ str(int(i)) +\")\"  )\n",
    "        class_labels.append(str(classlist[i])+\"-\"+str(classlist[i+1]) )\n",
    "  \n",
    "    summary[\"Scaled_number\"] = (summary[\"n_ratings\"]-summary[\"n_ratings\"].min())/(summary[\"n_ratings\"].max()-summary[\"n_ratings\"].min()) + 1.5\n",
    "    summary['Bucket_Names'] = class_labels\n",
    "  \n",
    "    # making plot\n",
    "    x = summary[\"Bucket_Names\"]\n",
    "    y1 = summary[\"avg_rating\"]\n",
    "    y2 = summary[\"n_ratings\"]\n",
    "    err = summary[\"stddev_rating\"]  \n",
    "\n",
    "    # Plot scatter here\n",
    "    plt.rcParams[\"figure.figsize\"] = [summary.shape[0]+2, 6.0]\n",
    "    plt.rcParams[\"figure.autolayout\"] = True\n",
    "    fig, ax1 = plt.subplots()\n",
    "\n",
    "    bar = ax1.bar(x, y1, color = \"#262261\")\n",
    "    ax1.errorbar(x, y1, yerr=err, fmt=\"o\", color=\"#EE4036\")\n",
    "    ax1.set(ylim=(0, 7))\n",
    "  \n",
    "    #ax1.bar_label(bar , fmt='%.2f', label_type='edge')  \n",
    "    def barlabel(x_list,y_list):\n",
    "        for i in range(len(x_list)):\n",
    "            ax1.text(i,y_list[i] + 0.2,y_list[i], ha = 'center',\n",
    "  \t\t\t         fontdict=dict(size=10),\n",
    "  \t\t\t         bbox=dict(facecolor='#262261', alpha=0.2)         \n",
    "  \t\t\t        )\n",
    "    barlabel(summary[\"Bucket_Names\"].tolist() ,summary[\"avg_rating\"].round(2).tolist())\n",
    "  \n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.scatter(x, y2, s=summary[\"Scaled_number\"]*500, c = '#FAAF40')  \n",
    "    ax2.set(ylim=(0, summary[\"n_ratings\"].max()*1.15))\n",
    "    def scatterlabel(x_list,y_list):\n",
    "  \t    for i in range(len(x_list)):\n",
    "  \t\t    ax2.text(i,y_list[i] + 15000,y_list[i], ha = 'center',\n",
    "  \t\t\t\t\t fontdict=dict(size=10),\n",
    "                     bbox=dict(facecolor='#FAAF40', alpha=0.5)\n",
    "  \t\t\t\t\t)\n",
    "    scatterlabel(summary[\"Bucket_Names\"].tolist() ,summary[\"n_ratings\"].tolist())\n",
    "  \n",
    "    # giving labels to the axises\n",
    "    ax1.set_xlabel(bucketizer.getOutputCol(), fontdict=dict(size=14)) \n",
    "    ax1.set_ylabel(\"Average Ratings\",fontdict=dict(size=14))\n",
    "  \n",
    "    # secondary y-axis label\n",
    "    ax2.set_ylabel('Number of Ratings',fontdict=dict(size=14))\n",
    "  \n",
    "    #plot Title\n",
    "    plt.title('Bucketwise average ratings and number of ratings for \\n'+bucketizer.getInputCol(), \n",
    "              fontdict=dict(size=14)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da73721",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bucket_col_print_summary(df, splits, inputCol, outputCol):\n",
    "    \"\"\"\n",
    "    Given a numerical column in a data frame, adds a bucketized version of the column to the data frame, according to splits provided.\n",
    "    Also prints a summary of ratings seen in each bucket made.\n",
    "\n",
    "    Inputs \n",
    "    Argument 1: Data Frame \n",
    "    Argument 2: Values at which the column will be split\n",
    "    Argument 3: Name of the input column (numerical column)\n",
    "    Argument 4: Name of the output column (bucketized numerical column)\n",
    "\n",
    "    Output: \n",
    "    1) New dataframe with the output column added\n",
    "    2) Bucketizer object trained from the input column \n",
    "    3) Pandas dataframe with summary statistics for ratings seen in buckets of the output column\n",
    "    Also plots summary statistics for ratings seen in buckets of the output column\n",
    "    \"\"\"\n",
    "\n",
    "    # Dropping bucket if it already exists\n",
    "    if outputCol in df.columns:\n",
    "        df = df.drop(outputCol)\n",
    "\n",
    "    # Training bucketizer\n",
    "    bucketizer = Bucketizer(splits = splits,\n",
    "                            inputCol  = inputCol,\n",
    "                            outputCol = outputCol)\n",
    "    \n",
    "    df = bucketizer.setHandleInvalid(\"keep\").transform(df)\n",
    "\n",
    "    # Printing meta information on buckets created\n",
    "    print(\"Added bucketized column {}\".format(outputCol))\n",
    "    print(\"\")\n",
    "    print(\"Bucketing done for split definition: {}\".format(splits))\n",
    "    print(\"\")  \n",
    "    print(\"Printing summary statistics for ratings in buckets below:\")\n",
    "\n",
    "    # Creating a summary statistics dataframe and passing it to the plotting function\n",
    "    summary =  (df\n",
    "                .groupBy(outputCol)\n",
    "                .agg(F.avg('rating').alias('avg_rating'),\n",
    "                     F.stddev('rating').alias('stddev_rating'),\n",
    "                     F.count('rating').alias('n_ratings'))\n",
    "                .sort(outputCol)\n",
    "                .toPandas())\n",
    "  \n",
    "    plot_bucketwise_statistics(summary,bucketizer)\n",
    "  \n",
    "    return df, bucketizer, summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71bd221c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_column_distribution_summary(df, col_name):\n",
    "    \"\"\"\n",
    "    Takes a column in a data frame and prints the summary statistics (average, standard deviation, count and distinct count) for all unique values in that column.\n",
    "  \n",
    "    Inputs \n",
    "    Argument 1: Dataframe \n",
    "    Argument 2: Name of the column\n",
    "  \n",
    "    Output\n",
    "    Returns nothing \n",
    "    Prints a Dataframe with summary statistics\n",
    "    \"\"\"\n",
    "    print(df\n",
    "          .groupBy(col_name)\n",
    "          .agg(F.avg('rating').alias('avg_rating'),\n",
    "               F.stddev('rating').alias('stddev_rating'),\n",
    "               F.count('rating').alias('n_ratings'),\n",
    "               F.countDistinct('id').alias('n_recipes'))\n",
    "          .sort(F.col(col_name).asc())\n",
    "          .show(50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79011c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_n_items_satisfying_condition (df, condition, aggregation_level = \"recipe\"):\n",
    "    \"\"\"\n",
    "    Given a condition, find the number of recipes / reviews that match the condition.\n",
    "    Also calculates the percentage of such recipes / reviews as a percentage of all recipes / reviews.\n",
    "  \n",
    "    Inputs \n",
    "    Argument 1: Dataframe \n",
    "    Argument 2: Logical expression describing a condition, string type. eg: \"minutes == 0\"\n",
    "    Argument 3: Aggregation level for determining \"items\", either  \"recipe\" or \"review\". Default value == \"recipe\"\n",
    "  \n",
    "    Output: Returns no object.\n",
    "    Prints the following:\n",
    "    1) Number of recipes / reviews that satisfy the condition\n",
    "    2) Total number of recipes / reviews in the dataframe\n",
    "    3) Percentage of recipes / reviews that satisfy the condition\n",
    "    \"\"\"\n",
    "    # Find out num rows satisfying the condition\n",
    "    if aggregation_level == \"recipe\": \n",
    "        number_of_rows_satisfying_condition = (df\n",
    "                                             .filter(condition)\n",
    "                                             .agg(F.countDistinct(\"id\"))).first()[0]\n",
    "      \n",
    "        n_rows_total = (df.agg(F.countDistinct(\"id\"))).first()[0]\n",
    "    if aggregation_level == \"review\":\n",
    "        number_of_rows_satisfying_condition = (df\n",
    "                                             .filter(condition)\n",
    "                                             .agg(F.countDistinct(\"id\",\"user_id\"))).first()[0]\n",
    "        n_rows_total = (df.agg(F.countDistinct(\"id\",\"user_id\"))).first()[0]\n",
    "  \n",
    "    # Find out % rows satisfying the conditon and print a properly formatted output\n",
    "    perc_rows = round(number_of_rows_satisfying_condition * 100/ n_rows_total, 2)\n",
    "    print('Condition String                   : \"{}\"'.format(condition))\n",
    "    print(\"Num {}s Satisfying Condition   : {} [{}%]\".format(aggregation_level.title(), number_of_rows_satisfying_condition, perc_rows))\n",
    "    print(\"Total Num {}s                  : {}\".format(aggregation_level.title(), n_rows_total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec89e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_OHE_columns (df, n_name_list):\n",
    "    \"\"\"\n",
    "    Given a list of tags, creates one hot encoded columns for each tag. \n",
    "  \n",
    "    Input\n",
    "    Argument 1: Dataframe in which the function will add the new columns\n",
    "    Argument 2: list of tags\n",
    "  \n",
    "    Output\n",
    "    Prints the names of columns that have been added \n",
    "    Returns the modified dataframe \n",
    "    \"\"\"\n",
    "    for name in n_name_list:\n",
    "        df = (df.withColumn(\"has_tag_\"+name, F.when(F.array_contains(df.tags, name), 1).otherwise(0)))\n",
    "        print (\"added column: has_tag_\"+name)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f974fc0",
   "metadata": {},
   "source": [
    "## Read the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6716e7a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "interaction_level_df = spark.read.parquet(\"s3://demobucketpavi/foodreceipe/interaction_level_df_postEDA.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48ba87c0",
   "metadata": {},
   "source": [
    "## Adding user level average features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e70465",
   "metadata": {},
   "outputs": [],
   "source": [
    "partition = Window.partitionBy(\"user_id\")\n",
    "\n",
    "interaction_level_df = (interaction_level_df\n",
    "                        .withColumn(\"user_avg_rating\",\n",
    "                                    F.avg(F.col(\"rating\")).over(partition))\n",
    "                        .withColumn(\"user_n_ratings\",\n",
    "                                    F.count(F.col(\"rating\")).over(partition))\n",
    "                        .withColumn(\"user_avg_years_betwn_review_and_submission\",\n",
    "                                    F.avg(F.col(\"years_since_submission_on_review_date\")).over(partition))\n",
    "                        .withColumn(\"user_avg_prep_time_recipes_reviewed\",\n",
    "                                    F.avg(F.col(\"minutes\")).over(partition))\n",
    "                        .withColumn(\"user_avg_n_steps_recipes_reviewed\",\n",
    "                                    F.avg(F.col(\"n_steps\")).over(partition))\n",
    "                        .withColumn(\"user_avg_n_ingredients_recipes_reviewed\",\n",
    "                                    F.avg(F.col(\"n_ingredients\")).over(partition)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab88a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "nutrition_cols = ['calories',\n",
    "                  'total_fat_per_100_cal',\n",
    "                  'sugar_per_100_cal',\n",
    "                  'sodium_per_100_cal',\n",
    "                  'protein_per_100_cal',\n",
    "                  'saturated_fat_per_100_cal',\n",
    "                  'carbohydrates_per_100_cal']\n",
    "\n",
    "for nutri_col in nutrition_cols:\n",
    "    interaction_level_df = (interaction_level_df\n",
    "                            .withColumn(\"user_avg_{}_recipes_reviewed\".format(nutri_col),\n",
    "                                        F.avg(F.col(nutri_col)).over(partition)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12589059",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Code check cell\n",
    "# Do not edit cells with assert commands\n",
    "# If an error is shown after running this cell, please recheck your code. \n",
    "\n",
    "assert(round(interaction_level_df.filter('user_id == 601529').select('user_avg_rating').first()[0], 2) == 4.22)\n",
    "assert(interaction_level_df.filter('user_id == 601529').select('user_n_ratings').first()[0] == 27)\n",
    "assert(round(interaction_level_df.filter('user_id == 601529').select('user_avg_years_betwn_review_and_submission').first()[0], 2) == 3.51)\n",
    "assert(interaction_level_df.filter('user_id == 233044').select('user_avg_prep_time_recipes_reviewed').first()[0] == 50.3)\n",
    "assert(interaction_level_df.filter('user_id == 233044').select('user_avg_n_steps_recipes_reviewed').first()[0] == 8.8)\n",
    "assert(interaction_level_df.filter('user_id == 233044').select('user_avg_n_ingredients_recipes_reviewed').first()[0] == 8.2)\n",
    "assert(round(interaction_level_df.filter('user_id == 233044').select('user_avg_total_fat_per_100_cal_recipes_reviewed').first()[0]) == 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7513b134",
   "metadata": {},
   "source": [
    "**More Features:**\n",
    "\n",
    "high_ratings = 5 rating\n",
    "\n",
    "- `user_avg_years_betwn_review_and_submission_high_ratings`\n",
    "- `user_avg_prep_time_recipes_reviewed_high_ratings`\n",
    "- `user_avg_n_steps_recipes_reviewed_high_ratings`\n",
    "- `user_avg_n_ingredients_recipes_reviewed_high_ratings`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa15a2fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "interaction_level_df = (interaction_level_df\n",
    "                        .withColumn(\"ind_5_rating\",\n",
    "                                    F.when(interaction_level_df[\"rating\"] != 5, None)\n",
    "                                     .otherwise(1))\n",
    "                        .withColumn(\"years_since_submission_on_review_date_5_ratings\",\n",
    "                                    F.when(interaction_level_df[\"rating\"] != 5, None)\n",
    "                                     .otherwise(F.col(\"years_since_submission_on_review_date\")))\n",
    "                        .withColumn(\"minutes_5_ratings\",\n",
    "                                    F.when(interaction_level_df[\"rating\"] != 5, None)\n",
    "                                     .otherwise(F.col(\"minutes\")))\n",
    "                        .withColumn(\"n_steps_5_ratings\",\n",
    "                                    F.when(interaction_level_df[\"rating\"] != 5, None)\n",
    "                                     .otherwise(F.col(\"n_steps\")))\n",
    "                        .withColumn(\"n_ingredients_5_ratings\",\n",
    "                                    F.when(interaction_level_df[\"rating\"] != 5, None)\n",
    "                                     .otherwise(F.col(\"n_ingredients\"))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4daa41ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "partition = Window.partitionBy(\"user_id\")\n",
    "\n",
    "interaction_level_df = (interaction_level_df\n",
    "                        .withColumn(\"user_n_5_ratings\",\n",
    "                                    F.sum(F.col(\"ind_5_rating\")).over(partition))\n",
    "                        .withColumn(\"user_avg_years_betwn_review_and_submission_5_ratings\",\n",
    "                                    F.avg(F.col(\"years_since_submission_on_review_date_5_ratings\")).over(partition))\n",
    "                        .withColumn(\"user_avg_prep_time_recipes_reviewed_5_ratings\",\n",
    "                                    F.avg(F.col(\"minutes_5_ratings\")).over(partition))\n",
    "                        .withColumn(\"user_avg_n_steps_recipes_reviewed_5_ratings\",\n",
    "                                    F.avg(F.col(\"n_steps_5_ratings\")).over(partition))\n",
    "                        .withColumn(\"user_avg_n_ingredients_recipes_reviewed_5_ratings\",\n",
    "                                    F.avg(F.col(\"n_ingredients_5_ratings\")).over(partition)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49be153d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for nutri_col in nutrition_cols:\n",
    "    interaction_level_df = (interaction_level_df\n",
    "                            .withColumn(\"{}_5_ratings\".format(nutri_col),\n",
    "                                        F.when(interaction_level_df[\"rating\"] != 5, None)\n",
    "                                         .otherwise(F.col(nutri_col))))\n",
    "    interaction_level_df = (interaction_level_df\n",
    "                            .withColumn(\"user_avg_{}_recipes_reviewed_5_ratings\".format(nutri_col),\n",
    "                                        F.avg(F.col(\"{}_5_ratings\".format(nutri_col))).over(partition)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ec5f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check - All rows with ratings should have non-null values in corresponding user_avg_5_ratings columns\n",
    "\n",
    "assert(interaction_level_df\n",
    "       .filter(\"rating == 5\")\n",
    "       .filter(interaction_level_df.user_n_5_ratings.isNull() |\n",
    "               interaction_level_df.user_avg_years_betwn_review_and_submission_5_ratings.isNull() |\n",
    "               interaction_level_df.user_avg_prep_time_recipes_reviewed_5_ratings.isNull() |\n",
    "               interaction_level_df.user_avg_n_steps_recipes_reviewed_5_ratings.isNull() |\n",
    "               interaction_level_df.user_avg_n_ingredients_recipes_reviewed_5_ratings.isNull())\n",
    "       .count() == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34bcdbfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check values for a given user id\n",
    "\n",
    "assert(interaction_level_df.filter('user_id == 233044').select('user_n_5_ratings').first()[0] == 7)\n",
    "assert(round(interaction_level_df.filter('user_id == 233044').select('user_avg_years_betwn_review_and_submission_5_ratings').first()[0], 2) == 2.24)\n",
    "assert(round(interaction_level_df.filter('user_id == 233044').select('user_avg_prep_time_recipes_reviewed_5_ratings').first()[0]) == 46)\n",
    "assert(round(interaction_level_df.filter('user_id == 233044').select('user_avg_n_steps_recipes_reviewed_5_ratings').first()[0], 2) == 7.29)\n",
    "assert(round(interaction_level_df.filter('user_id == 233044').select('user_avg_n_ingredients_recipes_reviewed_5_ratings').first()[0], 2) == 6.86)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b122a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "interaction_level_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d947f240",
   "metadata": {},
   "source": [
    "## Tags level EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0537107",
   "metadata": {},
   "outputs": [],
   "source": [
    "interaction_tag_level_df = interaction_level_df.withColumn('individual_tag',F.explode('tags'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a15d9938",
   "metadata": {},
   "outputs": [],
   "source": [
    "tags_ratings_summary = (interaction_tag_level_df\n",
    "                        .groupBy('individual_tag').agg(F.avg('rating').alias('avg_user_rating'),\n",
    "#                                                      F.max('rating').alias('max_user_rating'),\n",
    "#                                                      F.min('rating').alias('min_user_rating'),\n",
    "                                                       F.count('rating').alias('n_user_ratings'),\n",
    "                                                       F.countDistinct('id').alias('n_recipes')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aea4dfa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "interactions, recipes  =  interaction_level_df.count(), interaction_level_df.agg(F.countDistinct('id')).first()[0]\n",
    "\n",
    "tags_ratings_summary = (tags_ratings_summary.withColumn(\"in_percent_recipies\", F.col (\"n_recipes\")/F.lit(recipes))\n",
    "                                            .withColumn(\"in_percent_interactions\", F.col (\"n_user_ratings\")/F.lit(interactions)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b73f1b2",
   "metadata": {},
   "source": [
    "#### 1. Top ```n``` most rated tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8969bb9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tags_ratings_summary.sort(F.col(\"n_user_ratings\").desc()).show(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d3b60e",
   "metadata": {},
   "source": [
    "Drop tags present in majority of recipes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8601fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tags_ratings_summary = tags_ratings_summary.filter(tags_ratings_summary.in_percent_interactions < 0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d7c6535",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_most_frequent_tags = tags_ratings_summary.sort(F.col(\"n_user_ratings\").desc())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a68f45de",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_quantiles(df = top_most_frequent_tags , \n",
    "              col_name = 'in_percent_interactions', \n",
    "              quantiles_list = [0.01,0.25,0.5, 0.75,0.8,0.85,0.9,0.95, 0.99])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc86bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep tags appearing in the top 5 percentile \n",
    "top_most_frequent_tags = top_most_frequent_tags.filter(\"in_percent_interactions > 0.16\")\n",
    "\n",
    "top_most_frequent_tags.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a80d5598",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_frequent_tags_list = [data[0] for data in top_most_frequent_tags.select('individual_tag').collect()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0323df86",
   "metadata": {},
   "outputs": [],
   "source": [
    "interaction_level_df = add_OHE_columns (interaction_level_df, top_frequent_tags_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33744553",
   "metadata": {},
   "source": [
    "#### 2.  Bottom ```n``` least rated tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b02c0de",
   "metadata": {},
   "outputs": [],
   "source": [
    "tags_ratings_summary.sort(F.col(\"n_user_ratings\").asc()).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28edd839",
   "metadata": {},
   "source": [
    "The above tags are present in 1 recipe in over two hundred thousand. The features we create based on these tags will not teach the model new information. If these tags were one hot encoded, the entire column would be filled with zeros, and only a few rows will have 1s. One hot encoding of these tags is not a good idea. If you come up with an encoding that captures the rarity of these tags, only then can you add these tags to the analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3bacac2",
   "metadata": {},
   "source": [
    "#### 3. Top ```n``` rated tags "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f4391c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tags_ratings_summary.sort(F.col(\"avg_user_rating\").desc()).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ba743fa",
   "metadata": {},
   "source": [
    "Top rated tags have low number of ratings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c62424",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_quantiles (tags_ratings_summary, \"n_user_ratings\", quantiles_list = [0.01, 0.05, 0.1, 0.15, 0.2, 0.25, 0.5, 0.75, 0.99])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec928b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tags_ratings_summary = tags_ratings_summary.filter(tags_ratings_summary.n_user_ratings > 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f52df5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_rated_tags_df = tags_ratings_summary.sort(F.col(\"avg_user_rating\").desc())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "558d9eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_quantiles(df = top_rated_tags_df , \n",
    "              col_name = 'avg_user_rating', \n",
    "              quantiles_list = [0.01,0.25,0.5, 0.75,0.8,0.85,0.9,0.95, 0.99])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a0ca52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep tags above 95 percentile\n",
    "top_rated_tags_df = top_rated_tags_df.filter(\"avg_user_rating > 4.53\")\n",
    "\n",
    "top_rated_tags_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06ccccb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_rated_tags_list = [data[0] for data in top_rated_tags_df.select('individual_tag').collect()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cfe9edf",
   "metadata": {},
   "source": [
    "Check if any of the current tags have been added earlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaafc25c",
   "metadata": {},
   "outputs": [],
   "source": [
    "set(top_frequent_tags_list) & set(top_rated_tags_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c5a490",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_added_columns_set = set(top_frequent_tags_list).union(set(top_rated_tags_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5693caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "interaction_level_df = add_OHE_columns (interaction_level_df, top_rated_tags_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de06488a",
   "metadata": {},
   "source": [
    "#### 3. Bottom ```n``` rated tags "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f3abff",
   "metadata": {},
   "outputs": [],
   "source": [
    "bottom_rated_tags_df = tags_ratings_summary.sort(F.col(\"avg_user_rating\").asc())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d21ffd36",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_quantiles (bottom_rated_tags_df, \"avg_user_rating\", quantiles_list = [0.01, 0.02, 0.03, 0.04, 0.05, 0.1, 0.15, 0.2, 0.25, 0.5, 0.75, 0.99])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2166012b",
   "metadata": {},
   "outputs": [],
   "source": [
    "bottom_rated_tags_df = bottom_rated_tags_df.filter(\"avg_user_rating < 4.00\")\n",
    "\n",
    "bottom_rated_tags_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad52943b",
   "metadata": {},
   "outputs": [],
   "source": [
    "bottom_rated_tags_list = [data[0] for data in bottom_rated_tags_df.select('individual_tag').collect()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b5caf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_added_columns_set & set(bottom_rated_tags_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b044ac62",
   "metadata": {},
   "outputs": [],
   "source": [
    "interaction_level_df =  add_OHE_columns(interaction_level_df, bottom_rated_tags_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fedadf63",
   "metadata": {},
   "source": [
    "## Final DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dffa017",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(interaction_level_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c878b1df",
   "metadata": {},
   "outputs": [],
   "source": [
    "interaction_level_df.write.mode('overwrite').parquet('s3://demobucketpavi/foodreceipe/interaction_level_df_ModelReady.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c83a1641",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "93a655ab",
   "metadata": {},
   "source": [
    "## <center> THANK YOU `ASSIGNMENT` COMPLETED </center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88be7438",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
